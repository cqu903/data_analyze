{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder,OneHotEncoder,Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import  matplotlib.pyplot as plt\n",
    "# import os\n",
    "# os.environ[\"PATH\"] += os.pathsep + \"D:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\\"\n",
    "# print(os.environ[\"PATH\"])\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./modeldata1.csv',encoding=\"gbk\")\n",
    "data['target'] = data['是否逾期'].map({\n",
    "    \"是\":1,\n",
    "    \"否\":0\n",
    "})\n",
    "data = data.drop(['是否逾期'],axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['closed_accounts', 'open_accounts', 'enquiry_alert', '人脸相似度', '年龄', '信用卡额度使用率', '信用卡数', '信用卡平均额度', '贷款数', '贷款平均额度', '平均分期金额', '平均期数', '90天内贷款查询次数', '180天内贷款查询次数', '120天内信用卡查询次数', '365天内信用卡查询次数', '历史近两年逾期最大天数', 'ULC33S', '近三个月逾期天数', 'G207O', 'dsr_before', '历史申请贷款查询数', '开户成功率', 'total_monthly_obligation']\n",
      "['申请时段', 'grade', '电话使用时长', '分区', '身份证号首字母']\n"
     ]
    }
   ],
   "source": [
    "import numbers\n",
    "allFeatures = list(data.columns)\n",
    "allFeatures.remove('target')\n",
    "\n",
    "numerical_var = []\n",
    "for col in allFeatures:\n",
    "    if len(set(data[col])) == 1:\n",
    "        print('delete {} from the dataset because it is a constant'.format(col))\n",
    "        del data[col]\n",
    "        allFeatures.remove(col)\n",
    "    else:\n",
    "        uniq_valid_vals = [i for i in data[col] if i == i]\n",
    "        uniq_valid_vals = list(set(uniq_valid_vals))\n",
    "        if len(uniq_valid_vals) >= 6 and isinstance(uniq_valid_vals[0], numbers.Real):\n",
    "            numerical_var.append(col)\n",
    "\n",
    "categorical_var = [i for i in allFeatures if i not in numerical_var]\n",
    "print(numerical_var)\n",
    "print(categorical_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    \n",
    "    #label = data['target']\n",
    "    #data = data.drop(['target'],axis=1)\n",
    "    #特征选择\n",
    "    #考虑相关性，可以去除几个属性，但因为总体的属性不多，暂不处理\n",
    "    #特征处理\n",
    "    for i in range(len(numerical_var)):\n",
    "        data[numerical_var[i]] = MinMaxScaler().fit_transform(data[numerical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "\n",
    "    #处理离散值数值化\n",
    "    for i in range(len(categorical_var)):\n",
    "        if categorical_var[i] == \"申请时段\":\n",
    "            data[categorical_var[i]] = [map_apply(s) for s in data[\"申请时段\"].values]\n",
    "            data[categorical_var[i]] = MinMaxScaler().fit_transform(data[categorical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        elif categorical_var[i] == \"grade\":\n",
    "            data[categorical_var[i]] = [map_grade(w) for w in data['grade'].values]\n",
    "            data[categorical_var[i]] = MinMaxScaler().fit_transform(data[categorical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            data[categorical_var[i]] = LabelEncoder().fit_transform(data[categorical_var[i]])\n",
    "            #对于labelEncoder的情况下还需要进行归一化处理\n",
    "            data[categorical_var[i]] = MinMaxScaler().fit_transform(data[categorical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "\n",
    "    return data\n",
    "\n",
    "def map_apply(s):\n",
    "    d = dict([(\"0点\",24),(\"1点\",1),(\"2点\",2),(\"3点\",3),(\"4点\",4),(\"5点\",5),(\"6点\",6),(\"7点\",7),(\"8点\",8),(\"9点\",9),(\"10点\",10),(\"11点\",11),\n",
    "             (\"12点\",12),(\"13点\",13),(\"14点\",14),(\"15点\",15),(\"16点\",16),(\"17点\",17),(\"18点\",18),(\"19点\",19),(\"20点\",20),(\"21点\",21),\n",
    "             (\"22点\",22),(\"23点\",23)])\n",
    "    return d.get(s,0)\n",
    "def map_grade(w):\n",
    "    d = dict([(\"AA\",9),(\"BB\",8),(\"CC\",7),(\"DD\",6),(\"EE\",5),(\"FF\",4),(\"GG\",3),(\"HH\",2),(\"II\",1)])\n",
    "    return d.get(w,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          申请时段  grade  closed_accounts  open_accounts  enquiry_alert  \\\n",
      "0     0.869565  0.125         0.325581       0.294118       0.248908   \n",
      "1     0.913043  0.125         0.023256       0.294118       0.275109   \n",
      "2     0.521739  0.000         0.674419       0.352941       0.310044   \n",
      "3     0.434783  0.000         0.465116       0.235294       0.253275   \n",
      "4     0.391304  0.125         0.046512       0.294118       0.196507   \n",
      "5     0.434783  0.000         0.302326       0.705882       0.327511   \n",
      "6     0.521739  0.000         0.162791       0.352941       0.401747   \n",
      "7     0.565217  0.375         0.046512       0.000000       0.096070   \n",
      "8     0.869565  0.125         0.255814       0.411765       0.419214   \n",
      "9     1.000000  0.000         0.116279       0.235294       0.266376   \n",
      "10    0.434783  0.000         0.069767       0.352941       0.327511   \n",
      "11    0.565217  0.000         0.162791       0.470588       0.358079   \n",
      "12    0.608696  0.000         0.069767       0.411765       0.205240   \n",
      "13    0.608696  0.000         0.023256       0.470588       0.235808   \n",
      "14    0.695652  0.000         0.046512       0.294118       0.170306   \n",
      "15    0.739130  0.000         0.604651       0.235294       0.222707   \n",
      "16    0.347826  0.000         0.093023       0.529412       0.510917   \n",
      "17    0.478261  0.375         0.232558       0.117647       0.048035   \n",
      "18    0.695652  0.000         0.116279       0.235294       0.135371   \n",
      "19    0.304348  0.125         0.023256       0.411765       0.161572   \n",
      "20    0.434783  0.000         0.302326       0.294118       0.235808   \n",
      "21    0.434783  0.000         0.232558       0.705882       0.537118   \n",
      "22    0.478261  0.000         0.325581       0.352941       0.296943   \n",
      "23    0.478261  0.000         0.093023       0.235294       0.266376   \n",
      "24    0.478261  0.500         0.000000       0.000000       0.034934   \n",
      "25    0.565217  0.000         0.232558       0.411765       0.423581   \n",
      "26    0.565217  0.000         0.348837       0.529412       0.157205   \n",
      "27    0.608696  0.000         0.046512       0.294118       0.043668   \n",
      "28    0.782609  0.125         0.093023       0.117647       0.048035   \n",
      "29    0.739130  0.000         0.255814       0.411765       0.275109   \n",
      "...        ...    ...              ...            ...            ...   \n",
      "1533  0.956522  0.125         0.093023       0.117647       0.279476   \n",
      "1534  0.000000  0.000         0.023256       0.411765       0.231441   \n",
      "1535  0.521739  0.000         0.023256       0.294118       0.296943   \n",
      "1536  0.739130  0.000         0.046512       0.411765       0.183406   \n",
      "1537  0.782609  0.000         0.209302       0.470588       0.537118   \n",
      "1538  0.782609  0.000         0.046512       0.470588       0.471616   \n",
      "1539  0.130435  0.875         0.209302       0.705882       0.454148   \n",
      "1540  1.000000  0.000         0.023256       0.235294       0.222707   \n",
      "1541  0.347826  0.000         0.302326       0.294118       0.432314   \n",
      "1542  0.913043  0.250         0.232558       0.176471       0.218341   \n",
      "1543  0.521739  0.000         0.279070       0.352941       0.305677   \n",
      "1544  0.565217  0.000         0.651163       0.470588       0.502183   \n",
      "1545  0.652174  0.125         0.255814       0.470588       0.323144   \n",
      "1546  1.000000  0.250         0.093023       0.117647       0.344978   \n",
      "1547  0.173913  0.875         0.046512       0.117647       0.039301   \n",
      "1548  0.782609  0.250         0.000000       0.176471       0.349345   \n",
      "1549  0.434783  0.125         0.418605       0.823529       0.606987   \n",
      "1550  0.478261  0.250         0.046512       0.117647       0.209607   \n",
      "1551  0.652174  0.000         0.395349       0.235294       0.244541   \n",
      "1552  0.304348  0.000         0.232558       0.235294       0.209607   \n",
      "1553  0.347826  0.125         0.348837       0.235294       0.358079   \n",
      "1554  0.478261  0.000         0.604651       0.705882       0.545852   \n",
      "1555  0.565217  0.125         0.116279       0.352941       0.144105   \n",
      "1556  0.608696  0.125         0.325581       0.352941       0.344978   \n",
      "1557  0.652174  0.000         0.255814       0.000000       0.157205   \n",
      "1558  0.478261  0.250         0.093023       0.352941       0.244541   \n",
      "1559  0.391304  0.250         0.000000       0.529412       0.248908   \n",
      "1560  0.217391  0.000         0.186047       0.235294       0.187773   \n",
      "1561  0.695652  0.125         0.046512       0.294118       0.558952   \n",
      "1562  0.869565  0.375         0.000000       0.529412       0.314410   \n",
      "\n",
      "         人脸相似度  电话使用时长        年龄        分区  信用卡额度使用率  \\\n",
      "0     0.419753     0.0  0.361702  0.176471  0.526882   \n",
      "1     0.679012     0.8  0.319149  0.823529  0.543011   \n",
      "2     0.666667     0.8  0.595745  0.117647  0.349462   \n",
      "3     0.691358     0.8  0.191489  0.411765  0.000000   \n",
      "4     0.617284     0.8  0.212766  0.411765  0.526882   \n",
      "5     0.839506     0.8  0.574468  0.882353  0.317204   \n",
      "6     0.580247     1.0  0.170213  0.941176  0.000000   \n",
      "7     0.938272     0.2  0.170213  0.941176  0.397849   \n",
      "8     0.740741     0.0  0.319149  0.235294  0.252688   \n",
      "9     0.703704     0.2  0.425532  0.235294  0.532258   \n",
      "10    0.444444     0.8  0.170213  1.000000  0.236559   \n",
      "11    0.481481     0.8  0.489362  0.529412  0.494624   \n",
      "12    0.790123     0.0  0.085106  0.823529  0.204301   \n",
      "13    0.382716     1.0  0.170213  0.823529  0.317204   \n",
      "14    0.790123     1.0  0.042553  0.882353  0.553763   \n",
      "15    0.567901     0.8  0.148936  0.529412  0.451613   \n",
      "16    0.790123     0.0  0.127660  0.411765  0.413978   \n",
      "17    0.666667     0.8  0.595745  0.117647  0.537634   \n",
      "18    0.543210     1.0  0.148936  0.176471  0.532258   \n",
      "19    0.407407     0.4  0.276596  0.294118  0.537634   \n",
      "20    0.777778     0.0  0.234043  0.823529  0.440860   \n",
      "21    0.753086     0.0  0.638298  0.411765  0.220430   \n",
      "22    0.654321     0.8  0.319149  1.000000  0.510753   \n",
      "23    0.753086     0.8  0.255319  0.470588  0.440860   \n",
      "24    0.876543     0.0  0.042553  0.117647  0.397849   \n",
      "25    0.493827     0.8  0.638298  0.294118  0.532258   \n",
      "26    0.469136     1.0  0.148936  0.411765  0.446237   \n",
      "27    0.703704     0.0  0.127660  0.235294  0.564516   \n",
      "28    0.432099     0.8  0.085106  0.882353  0.489247   \n",
      "29    0.407407     0.8  0.553191  0.882353  0.274194   \n",
      "...        ...     ...       ...       ...       ...   \n",
      "1533  0.777778     0.8  0.382979  0.941176  0.532258   \n",
      "1534  0.555556     1.0  0.063830  1.000000  0.510753   \n",
      "1535  0.654321     1.0  0.148936  0.941176  0.473118   \n",
      "1536  0.703704     0.8  0.085106  0.823529  0.532258   \n",
      "1537  0.518519     0.8  0.297872  0.882353  0.301075   \n",
      "1538  0.469136     0.0  0.127660  0.823529  0.543011   \n",
      "1539  0.629630     0.0  0.468085  0.000000  0.032258   \n",
      "1540  0.679012     0.2  0.191489  0.176471  0.548387   \n",
      "1541  0.703704     0.8  0.234043  0.352941  0.586022   \n",
      "1542  0.740741     0.8  0.574468  0.117647  0.166667   \n",
      "1543  0.691358     0.8  0.148936  0.176471  0.505376   \n",
      "1544  0.728395     0.8  0.425532  0.882353  0.306452   \n",
      "1545  0.000000     0.8  0.446809  0.294118  0.489247   \n",
      "1546  0.000000     0.8  0.170213  0.941176  0.376344   \n",
      "1547  0.666667     0.4  0.170213  0.352941  0.397849   \n",
      "1548  0.666667     0.2  0.106383  0.941176  0.526882   \n",
      "1549  0.691358     0.0  0.723404  0.000000  0.494624   \n",
      "1550  0.703704     0.6  0.085106  0.235294  0.451613   \n",
      "1551  0.666667     0.0  0.489362  0.882353  0.526882   \n",
      "1552  0.777778     0.8  0.170213  0.470588  0.516129   \n",
      "1553  0.000000     0.8  0.276596  0.470588  0.075269   \n",
      "1554  0.679012     0.8  0.361702  0.235294  0.134409   \n",
      "1555  0.814815     0.4  0.446809  0.470588  0.537634   \n",
      "1556  0.654321     0.8  0.765957  0.941176  0.370968   \n",
      "1557  0.654321     0.0  0.425532  0.176471  0.397849   \n",
      "1558  0.506173     0.6  0.148936  0.235294  0.091398   \n",
      "1559  0.641975     0.4  0.574468  0.470588  0.580645   \n",
      "1560  0.000000     0.6  0.276596  0.588235  0.655914   \n",
      "1561  0.000000     0.8  0.255319  0.705882  0.505376   \n",
      "1562  0.679012     0.8  0.510638  0.705882  0.532258   \n",
      "\n",
      "                ...             365天内信用卡查询次数  历史近两年逾期最大天数    ULC33S  近三个月逾期天数  \\\n",
      "0               ...                 0.222222     0.089109  0.053870  0.018018   \n",
      "1               ...                 0.000000     0.000000  0.066993  0.000000   \n",
      "2               ...                 0.111111     0.000000  0.075660  0.000000   \n",
      "3               ...                 0.000000     0.039604  0.009964  0.008008   \n",
      "4               ...                 0.000000     0.000000  0.058846  0.000000   \n",
      "5               ...                 0.055556     0.014851  0.067188  0.000000   \n",
      "6               ...                 0.000000     0.316832  0.060200  0.000000   \n",
      "7               ...                 0.000000     0.000000  0.034518  0.000000   \n",
      "8               ...                 0.000000     0.000000  0.146386  0.000000   \n",
      "9               ...                 0.722222     0.004950  0.017730  0.001001   \n",
      "10              ...                 0.055556     0.000000  0.044134  0.061061   \n",
      "11              ...                 0.055556     0.158416  0.165757  0.032032   \n",
      "12              ...                 0.055556     0.059406  0.013357  0.012012   \n",
      "13              ...                 0.166667     0.009901  0.097403  0.000000   \n",
      "14              ...                 0.611111     0.000000  0.025097  0.000000   \n",
      "15              ...                 0.055556     0.019802  0.044158  0.000000   \n",
      "16              ...                 0.444444     0.000000  0.034405  0.001001   \n",
      "17              ...                 0.055556     0.000000  0.006963  0.000000   \n",
      "18              ...                 0.000000     0.004950  0.035996  0.000000   \n",
      "19              ...                 0.000000     0.000000  0.078938  0.001001   \n",
      "20              ...                 0.000000     0.019802  0.018606  0.004004   \n",
      "21              ...                 0.111111     0.000000  0.038688  0.000000   \n",
      "22              ...                 0.000000     0.000000  0.038920  0.000000   \n",
      "23              ...                 0.000000     0.000000  0.025075  0.000000   \n",
      "24              ...                 0.000000     0.000000  0.034518  0.000000   \n",
      "25              ...                 0.000000     0.000000  0.120708  0.030030   \n",
      "26              ...                 0.388889     0.084158  0.022975  0.004004   \n",
      "27              ...                 0.166667     0.000000  0.018694  0.000000   \n",
      "28              ...                 0.055556     0.000000  0.005785  0.000000   \n",
      "29              ...                 0.166667     0.044554  0.091990  0.009009   \n",
      "...             ...                      ...          ...       ...       ...   \n",
      "1533            ...                 0.000000     0.000000  0.039473  0.000000   \n",
      "1534            ...                 0.222222     0.000000  0.059009  0.000000   \n",
      "1535            ...                 0.055556     0.148515  0.024419  0.001001   \n",
      "1536            ...                 0.111111     0.000000  0.028169  0.000000   \n",
      "1537            ...                 0.000000     0.039604  0.039638  0.004004   \n",
      "1538            ...                 1.000000     0.000000  0.077543  0.001001   \n",
      "1539            ...                 0.000000     0.000000  0.003913  0.000000   \n",
      "1540            ...                 0.000000     0.000000  0.042529  0.000000   \n",
      "1541            ...                 0.055556     0.064356  0.035895  0.013013   \n",
      "1542            ...                 0.055556     0.000000  0.067176  0.000000   \n",
      "1543            ...                 0.000000     0.034653  0.048683  0.007007   \n",
      "1544            ...                 0.000000     0.000000  0.158347  0.000000   \n",
      "1545            ...                 0.000000     0.123762  0.082991  0.030030   \n",
      "1546            ...                 0.000000     0.000000  0.034518  0.000000   \n",
      "1547            ...                 0.000000     0.000000  0.000000  0.000000   \n",
      "1548            ...                 0.000000     0.000000  0.052565  0.000000   \n",
      "1549            ...                 0.000000     0.148515  0.352921  0.001001   \n",
      "1550            ...                 0.000000     0.000000  0.024116  0.005005   \n",
      "1551            ...                 0.055556     0.000000  0.147801  0.000000   \n",
      "1552            ...                 0.000000     0.000000  0.043062  0.000000   \n",
      "1553            ...                 0.000000     0.000000  0.070729  0.000000   \n",
      "1554            ...                 0.166667     0.004950  0.109830  0.001001   \n",
      "1555            ...                 0.055556     0.000000  0.159692  0.000000   \n",
      "1556            ...                 0.000000     0.000000  0.022818  0.000000   \n",
      "1557            ...                 0.000000     0.000000  0.034518  0.012012   \n",
      "1558            ...                 0.666667     0.000000  0.013317  0.001001   \n",
      "1559            ...                 0.000000     0.000000  0.062588  0.000000   \n",
      "1560            ...                 1.000000     0.000000  0.016367  0.001001   \n",
      "1561            ...                 0.111111     0.000000  0.068259  0.001001   \n",
      "1562            ...                 0.000000     0.000000  0.141539  0.000000   \n",
      "\n",
      "         G207O  dsr_before   身份证号首字母  历史申请贷款查询数     开户成功率  \\\n",
      "0     0.086727    0.688679  1.000000   0.206186  0.083333   \n",
      "1     0.062535    0.424528  1.000000   0.061856  0.018889   \n",
      "2     0.136909    0.773585  0.272727   0.309278  0.122222   \n",
      "3     0.015911    0.075472  0.909091   0.278351  0.070000   \n",
      "4     0.075624    0.207547  0.909091   0.041237  0.138889   \n",
      "5     0.073855    0.943396  0.454545   0.288660  0.078889   \n",
      "6     0.101108    0.594340  0.909091   0.494845  0.018889   \n",
      "7     0.046168    0.301887  0.909091   0.216495  0.011111   \n",
      "8     0.077856    0.424528  1.000000   0.092784  0.036667   \n",
      "9     0.028692    0.273585  0.636364   0.319588  0.032222   \n",
      "10    0.042385    0.424528  0.909091   0.123711  0.027778   \n",
      "11    0.196988    0.377358  0.454545   0.391753  0.037778   \n",
      "12    0.033836    0.292453  0.727273   0.206186  0.050000   \n",
      "13    0.170984    0.509434  0.909091   0.185567  0.024444   \n",
      "14    0.022242    0.594340  0.909091   0.154639  0.030000   \n",
      "15    0.064219    0.481132  0.909091   0.268041  0.124444   \n",
      "16    0.046357    0.415094  0.909091   0.257732  0.017778   \n",
      "17    0.017294    0.132075  0.454545   0.061856  0.185556   \n",
      "18    0.041191    0.311321  0.909091   0.144330  0.071111   \n",
      "19    0.085639    0.575472  1.000000   0.051546  0.022222   \n",
      "20    0.036931    0.169811  0.818182   0.123711  0.138889   \n",
      "21    0.070178    0.698113  0.272727   0.474227  0.041111   \n",
      "22    0.047431    0.471698  1.000000   0.298969  0.057778   \n",
      "23    0.038082    0.301887  1.000000   0.144330  0.063333   \n",
      "24    0.046168    0.000000  0.909091   0.072165  0.000000   \n",
      "25    0.123553    0.830189  0.090909   0.113402  0.101111   \n",
      "26    0.043108    0.339623  0.909091   0.164948  0.090000   \n",
      "27    0.025042    0.283019  0.909091   0.010309  0.111111   \n",
      "28    0.006345    0.141509  0.727273   0.061856  0.036667   \n",
      "29    0.105488    0.764151  0.454545   0.082474  0.166667   \n",
      "...        ...         ...       ...        ...       ...   \n",
      "1533  0.035408    0.264151  1.000000   0.020619  0.277778   \n",
      "1534  0.068009    0.764151  0.727273   0.041237  0.138889   \n",
      "1535  0.025393    0.226415  0.909091   0.154639  0.036667   \n",
      "1536  0.028516    0.481132  0.909091   0.092784  0.062222   \n",
      "1537  0.075182    0.726415  0.636364   0.144330  0.103333   \n",
      "1538  0.073097    0.650943  0.818182   0.164948  0.034444   \n",
      "1539  0.004078    0.056604  0.454545   0.000000  0.000000   \n",
      "1540  0.055179    0.245283  0.909091   0.041237  0.083333   \n",
      "1541  0.060324    0.452830  0.909091   0.164948  0.083333   \n",
      "1542  0.070437    0.603774  0.454545   0.041237  0.277778   \n",
      "1543  0.073926    0.500000  0.909091   0.216495  0.095556   \n",
      "1544  0.182565    0.613208  1.000000   0.402062  0.096667   \n",
      "1545  0.282325    0.952830  0.636364   0.175258  0.091111   \n",
      "1546  0.029765    0.198113  0.818182   0.144330  0.032222   \n",
      "1547  0.000000    0.000000  0.818182   0.020619  0.000000   \n",
      "1548  0.057053    0.481132  0.727273   0.000000  0.000000   \n",
      "1549  0.403422    0.707547  0.090909   0.123711  0.046667   \n",
      "1550  0.020796    0.188679  0.909091   0.020619  0.166667   \n",
      "1551  0.114366    0.556604  0.454545   0.195876  0.116667   \n",
      "1552  0.082320    0.688679  0.909091   0.164948  0.083333   \n",
      "1553  0.083885    0.462264  1.000000   0.134021  0.120000   \n",
      "1554  0.237786    0.801887  1.000000   0.453608  0.085556   \n",
      "1555  0.125462    0.424528  1.000000   0.010309  0.111111   \n",
      "1556  0.059320    0.396226  0.272727   0.092784  0.234444   \n",
      "1557  0.046168    0.000000  1.000000   0.144330  0.055556   \n",
      "1558  0.036264    0.330189  0.909091   0.020619  0.166667   \n",
      "1559  0.058682    0.311321  0.454545   0.010309  0.222222   \n",
      "1560  0.014114    0.207547  1.000000   0.092784  0.012222   \n",
      "1561  0.085976    0.575472  0.727273   0.051546  0.022222   \n",
      "1562  0.135225    0.632075  0.727273   0.010309  0.000000   \n",
      "\n",
      "      total_monthly_obligation  \n",
      "0                     0.219663  \n",
      "1                     0.160052  \n",
      "2                     0.340545  \n",
      "3                     0.042387  \n",
      "4                     0.192304  \n",
      "5                     0.185162  \n",
      "6                     0.252313  \n",
      "7                     0.000000  \n",
      "8                     0.197804  \n",
      "9                     0.073878  \n",
      "10                    0.107618  \n",
      "11                    0.488560  \n",
      "12                    0.086554  \n",
      "13                    0.000000  \n",
      "14                    0.057985  \n",
      "15                    0.161435  \n",
      "16                    0.117406  \n",
      "17                    0.049252  \n",
      "18                    0.104678  \n",
      "19                    0.216982  \n",
      "20                    0.094198  \n",
      "21                    0.176100  \n",
      "22                    0.120052  \n",
      "23                    0.097017  \n",
      "24                    0.007540  \n",
      "25                    0.307635  \n",
      "26                    0.109416  \n",
      "27                    0.064903  \n",
      "28                    0.021600  \n",
      "29                    0.206399  \n",
      "...                        ...  \n",
      "1533                  0.093212  \n",
      "1534                  0.170774  \n",
      "1535                  0.065767  \n",
      "1536                  0.073446  \n",
      "1537                  0.188448  \n",
      "1538                  0.000000  \n",
      "1539                  0.013247  \n",
      "1540                  0.139144  \n",
      "1541                  0.000000  \n",
      "1542                  0.179524  \n",
      "1543                  0.185335  \n",
      "1544                  0.000000  \n",
      "1545                  0.701617  \n",
      "1546                  0.079308  \n",
      "1547                  0.000000  \n",
      "1548                  0.146546  \n",
      "1549                  1.000000  \n",
      "1550                  0.057207  \n",
      "1551                  0.284981  \n",
      "1552                  0.206018  \n",
      "1553                  0.212659  \n",
      "1554                  0.589088  \n",
      "1555                  0.315106  \n",
      "1556                  0.152131  \n",
      "1557                  0.003182  \n",
      "1558                  0.095322  \n",
      "1559                  0.000000  \n",
      "1560                  0.037959  \n",
      "1561                  0.217812  \n",
      "1562                  0.000000  \n",
      "\n",
      "[1563 rows x 29 columns]\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       1\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      1\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      1\n",
      "23      1\n",
      "24      1\n",
      "25      1\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "1533    0\n",
      "1534    0\n",
      "1535    0\n",
      "1536    0\n",
      "1537    0\n",
      "1538    0\n",
      "1539    0\n",
      "1540    0\n",
      "1541    0\n",
      "1542    0\n",
      "1543    0\n",
      "1544    0\n",
      "1545    0\n",
      "1546    0\n",
      "1547    0\n",
      "1548    0\n",
      "1549    0\n",
      "1550    0\n",
      "1551    0\n",
      "1552    0\n",
      "1553    0\n",
      "1554    0\n",
      "1555    0\n",
      "1556    0\n",
      "1557    0\n",
      "1558    0\n",
      "1559    0\n",
      "1560    0\n",
      "1561    0\n",
      "1562    0\n",
      "Name: target, Length: 1563, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roy/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X = preprocessing()\n",
    "Y = X['target']\n",
    "X = X.drop(['target'],axis=1)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doMetrics(Y_true,Y_predict,clf_name):\n",
    "    from sklearn.metrics import accuracy_score,recall_score,f1_score,confusion_matrix\n",
    "    print(clf_name,\"_Acc:\",accuracy_score(Y_true,Y_predict))\n",
    "    print(clf_name,\"_recall:\",recall_score(Y_true,Y_predict))\n",
    "    print(clf_name,\"_f1-score:\",f1_score(Y_true,Y_predict))\n",
    "    print(confusion_matrix(Y_true,Y_predict,labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1094, 29)\n",
      "115\n",
      "(469, 29)\n",
      "RandomForestClassifier _Acc: 0.782515991471\n",
      "RandomForestClassifier _recall: 0.56\n",
      "RandomForestClassifier _f1-score: 0.354430379747\n",
      "[[339  80]\n",
      " [ 22  28]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "print(X_train.shape)\n",
    "print(Y_train.sum())\n",
    "print(X_test.shape)\n",
    "# tree = DecisionTreeClassifier(random_state=0, class_weight='balanced', min_weight_fraction_leaf=0.05,splitter='random')\n",
    "# tree.fit(X_train,Y_train)\n",
    "# tree.score(X_test,Y_test)\n",
    "# Y_predict = tree.predict(X_test)\n",
    "# doMetrics(Y_test,Y_predict,\"DecisionTreeClassifier\")\n",
    "# print(tree.feature_importances_)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=1000,n_jobs=4,class_weight='balanced', min_weight_fraction_leaf=0.1)\n",
    "rfc.fit(X_train,Y_train)\n",
    "doMetrics(Y_test,rfc.predict(X_test),\"RandomForestClassifier\")\n",
    "plt.bar(range(len(rfc.feature_importances_)),rfc.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(n_estimators=1000,n_jobs=4,class_weight='balanced', min_weight_fraction_leaf=0.1\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\n",
    "          'max_depth':range(1,20,1)\n",
    "         }\n",
    "clf = RandomForestClassifier(class_weight='balanced',n_jobs=-1)\n",
    "gs = GridSearchCV(clf,params,cv=5,n_jobs=-1)\n",
    "gs.fit(X,Y)\n",
    "print(gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用降采样\n",
    "from imblearn.ensemble import EasyEnsemble\n",
    "ee = EasyEnsemble(random_state=0, n_subsets=10)\n",
    "X_resampled, y_resampled = ee.fit_sample(X, Y)\n",
    "print(X_resampled.shape)\n",
    "print(y_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "#对决策树进行画图和输出\n",
    "def plotTree(clf,clf_name,feature_names):\n",
    "    dot_data = export_graphviz(clf,\n",
    "                               out_file=None,\n",
    "                               feature_names=feature_names,\n",
    "                               class_names=[\"Good\",\"Bad\"],\n",
    "                               filled=True,\n",
    "                               rounded=True,\n",
    "                               special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    graph.write_pdf(clf_name + \".pdf\")\n",
    "plotTree(tree,\"tree\",X_train.columns.values)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
