{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder,OneHotEncoder,Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import  matplotlib.pyplot as plt\n",
    "# import os\n",
    "# os.environ[\"PATH\"] += os.pathsep + \"D:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\\"\n",
    "# print(os.environ[\"PATH\"])\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./modeldata1.csv',encoding=\"gbk\")\n",
    "data['target'] = data['是否逾期'].map({\n",
    "    \"是\":1,\n",
    "    \"否\":0\n",
    "})\n",
    "data = data.drop(['是否逾期'],axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['closed_accounts', 'open_accounts', 'enquiry_alert', '人脸相似度', '年龄', '信用卡额度使用率', '信用卡数', '信用卡平均额度', '贷款数', '贷款平均额度', '平均分期金额', '平均期数', '90天内贷款查询次数', '180天内贷款查询次数', '120天内信用卡查询次数', '365天内信用卡查询次数', '历史近两年逾期最大天数', 'ULC33S', '近三个月逾期天数', 'G207O', 'dsr_before', '历史申请贷款查询数', '开户成功率', 'total_monthly_obligation']\n",
      "['申请时段', 'grade', '电话使用时长', '分区', '身份证号首字母']\n"
     ]
    }
   ],
   "source": [
    "import numbers\n",
    "allFeatures = list(data.columns)\n",
    "allFeatures.remove('target')\n",
    "\n",
    "numerical_var = []\n",
    "for col in allFeatures:\n",
    "    if len(set(data[col])) == 1:\n",
    "        print('delete {} from the dataset because it is a constant'.format(col))\n",
    "        del data[col]\n",
    "        allFeatures.remove(col)\n",
    "    else:\n",
    "        uniq_valid_vals = [i for i in data[col] if i == i]\n",
    "        uniq_valid_vals = list(set(uniq_valid_vals))\n",
    "        if len(uniq_valid_vals) >= 6 and isinstance(uniq_valid_vals[0], numbers.Real):\n",
    "            numerical_var.append(col)\n",
    "\n",
    "categorical_var = [i for i in allFeatures if i not in numerical_var]\n",
    "print(numerical_var)\n",
    "print(categorical_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    \n",
    "    #label = data['target']\n",
    "    #data = data.drop(['target'],axis=1)\n",
    "    #特征选择\n",
    "    #考虑相关性，可以去除几个属性，但因为总体的属性不多，暂不处理\n",
    "    #特征处理\n",
    "    for i in range(len(numerical_var)):\n",
    "        data[numerical_var[i]] = MinMaxScaler().fit_transform(data[numerical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "\n",
    "    #处理离散值数值化\n",
    "    for i in range(len(categorical_var)):\n",
    "        if categorical_var[i] == \"申请时段\":\n",
    "            data[categorical_var[i]] = [map_apply(s) for s in data[\"申请时段\"].values]\n",
    "            data[categorical_var[i]] = MinMaxScaler().fit_transform(data[categorical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        elif categorical_var[i] == \"grade\":\n",
    "            data[categorical_var[i]] = [map_grade(w) for w in data['grade'].values]\n",
    "            data[categorical_var[i]] = MinMaxScaler().fit_transform(data[categorical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            data[categorical_var[i]] = LabelEncoder().fit_transform(data[categorical_var[i]])\n",
    "            #对于labelEncoder的情况下还需要进行归一化处理\n",
    "            data[categorical_var[i]] = MinMaxScaler().fit_transform(data[categorical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "\n",
    "    return data\n",
    "\n",
    "def map_apply(s):\n",
    "    d = dict([(\"0点\",24),(\"1点\",1),(\"2点\",2),(\"3点\",3),(\"4点\",4),(\"5点\",5),(\"6点\",6),(\"7点\",7),(\"8点\",8),(\"9点\",9),(\"10点\",10),(\"11点\",11),\n",
    "             (\"12点\",12),(\"13点\",13),(\"14点\",14),(\"15点\",15),(\"16点\",16),(\"17点\",17),(\"18点\",18),(\"19点\",19),(\"20点\",20),(\"21点\",21),\n",
    "             (\"22点\",22),(\"23点\",23)])\n",
    "    return d.get(s,0)\n",
    "def map_grade(w):\n",
    "    d = dict([(\"AA\",9),(\"BB\",8),(\"CC\",7),(\"DD\",6),(\"EE\",5),(\"FF\",4),(\"GG\",3),(\"HH\",2),(\"II\",1)])\n",
    "    return d.get(w,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          申请时段  grade  closed_accounts  open_accounts  enquiry_alert  \\\n",
      "0     0.869565  0.125         0.325581       0.294118       0.248908   \n",
      "1     0.913043  0.125         0.023256       0.294118       0.275109   \n",
      "2     0.521739  0.000         0.674419       0.352941       0.310044   \n",
      "3     0.434783  0.000         0.465116       0.235294       0.253275   \n",
      "4     0.391304  0.125         0.046512       0.294118       0.196507   \n",
      "5     0.434783  0.000         0.302326       0.705882       0.327511   \n",
      "6     0.521739  0.000         0.162791       0.352941       0.401747   \n",
      "7     0.565217  0.375         0.046512       0.000000       0.096070   \n",
      "8     0.869565  0.125         0.255814       0.411765       0.419214   \n",
      "9     1.000000  0.000         0.116279       0.235294       0.266376   \n",
      "10    0.434783  0.000         0.069767       0.352941       0.327511   \n",
      "11    0.565217  0.000         0.162791       0.470588       0.358079   \n",
      "12    0.608696  0.000         0.069767       0.411765       0.205240   \n",
      "13    0.608696  0.000         0.023256       0.470588       0.235808   \n",
      "14    0.695652  0.000         0.046512       0.294118       0.170306   \n",
      "15    0.739130  0.000         0.604651       0.235294       0.222707   \n",
      "16    0.347826  0.000         0.093023       0.529412       0.510917   \n",
      "17    0.478261  0.375         0.232558       0.117647       0.048035   \n",
      "18    0.695652  0.000         0.116279       0.235294       0.135371   \n",
      "19    0.304348  0.125         0.023256       0.411765       0.161572   \n",
      "20    0.434783  0.000         0.302326       0.294118       0.235808   \n",
      "21    0.434783  0.000         0.232558       0.705882       0.537118   \n",
      "22    0.478261  0.000         0.325581       0.352941       0.296943   \n",
      "23    0.478261  0.000         0.093023       0.235294       0.266376   \n",
      "24    0.478261  0.500         0.000000       0.000000       0.034934   \n",
      "25    0.565217  0.000         0.232558       0.411765       0.423581   \n",
      "26    0.565217  0.000         0.348837       0.529412       0.157205   \n",
      "27    0.608696  0.000         0.046512       0.294118       0.043668   \n",
      "28    0.782609  0.125         0.093023       0.117647       0.048035   \n",
      "29    0.739130  0.000         0.255814       0.411765       0.275109   \n",
      "...        ...    ...              ...            ...            ...   \n",
      "1533  0.956522  0.125         0.093023       0.117647       0.279476   \n",
      "1534  0.000000  0.000         0.023256       0.411765       0.231441   \n",
      "1535  0.521739  0.000         0.023256       0.294118       0.296943   \n",
      "1536  0.739130  0.000         0.046512       0.411765       0.183406   \n",
      "1537  0.782609  0.000         0.209302       0.470588       0.537118   \n",
      "1538  0.782609  0.000         0.046512       0.470588       0.471616   \n",
      "1539  0.130435  0.875         0.209302       0.705882       0.454148   \n",
      "1540  1.000000  0.000         0.023256       0.235294       0.222707   \n",
      "1541  0.347826  0.000         0.302326       0.294118       0.432314   \n",
      "1542  0.913043  0.250         0.232558       0.176471       0.218341   \n",
      "1543  0.521739  0.000         0.279070       0.352941       0.305677   \n",
      "1544  0.565217  0.000         0.651163       0.470588       0.502183   \n",
      "1545  0.652174  0.125         0.255814       0.470588       0.323144   \n",
      "1546  1.000000  0.250         0.093023       0.117647       0.344978   \n",
      "1547  0.173913  0.875         0.046512       0.117647       0.039301   \n",
      "1548  0.782609  0.250         0.000000       0.176471       0.349345   \n",
      "1549  0.434783  0.125         0.418605       0.823529       0.606987   \n",
      "1550  0.478261  0.250         0.046512       0.117647       0.209607   \n",
      "1551  0.652174  0.000         0.395349       0.235294       0.244541   \n",
      "1552  0.304348  0.000         0.232558       0.235294       0.209607   \n",
      "1553  0.347826  0.125         0.348837       0.235294       0.358079   \n",
      "1554  0.478261  0.000         0.604651       0.705882       0.545852   \n",
      "1555  0.565217  0.125         0.116279       0.352941       0.144105   \n",
      "1556  0.608696  0.125         0.325581       0.352941       0.344978   \n",
      "1557  0.652174  0.000         0.255814       0.000000       0.157205   \n",
      "1558  0.478261  0.250         0.093023       0.352941       0.244541   \n",
      "1559  0.391304  0.250         0.000000       0.529412       0.248908   \n",
      "1560  0.217391  0.000         0.186047       0.235294       0.187773   \n",
      "1561  0.695652  0.125         0.046512       0.294118       0.558952   \n",
      "1562  0.869565  0.375         0.000000       0.529412       0.314410   \n",
      "\n",
      "         人脸相似度  电话使用时长        年龄        分区  信用卡额度使用率  \\\n",
      "0     0.419753     0.0  0.361702  0.176471  0.526882   \n",
      "1     0.679012     0.8  0.319149  0.823529  0.543011   \n",
      "2     0.666667     0.8  0.595745  0.117647  0.349462   \n",
      "3     0.691358     0.8  0.191489  0.411765  0.000000   \n",
      "4     0.617284     0.8  0.212766  0.411765  0.526882   \n",
      "5     0.839506     0.8  0.574468  0.882353  0.317204   \n",
      "6     0.580247     1.0  0.170213  0.941176  0.000000   \n",
      "7     0.938272     0.2  0.170213  0.941176  0.397849   \n",
      "8     0.740741     0.0  0.319149  0.235294  0.252688   \n",
      "9     0.703704     0.2  0.425532  0.235294  0.532258   \n",
      "10    0.444444     0.8  0.170213  1.000000  0.236559   \n",
      "11    0.481481     0.8  0.489362  0.529412  0.494624   \n",
      "12    0.790123     0.0  0.085106  0.823529  0.204301   \n",
      "13    0.382716     1.0  0.170213  0.823529  0.317204   \n",
      "14    0.790123     1.0  0.042553  0.882353  0.553763   \n",
      "15    0.567901     0.8  0.148936  0.529412  0.451613   \n",
      "16    0.790123     0.0  0.127660  0.411765  0.413978   \n",
      "17    0.666667     0.8  0.595745  0.117647  0.537634   \n",
      "18    0.543210     1.0  0.148936  0.176471  0.532258   \n",
      "19    0.407407     0.4  0.276596  0.294118  0.537634   \n",
      "20    0.777778     0.0  0.234043  0.823529  0.440860   \n",
      "21    0.753086     0.0  0.638298  0.411765  0.220430   \n",
      "22    0.654321     0.8  0.319149  1.000000  0.510753   \n",
      "23    0.753086     0.8  0.255319  0.470588  0.440860   \n",
      "24    0.876543     0.0  0.042553  0.117647  0.397849   \n",
      "25    0.493827     0.8  0.638298  0.294118  0.532258   \n",
      "26    0.469136     1.0  0.148936  0.411765  0.446237   \n",
      "27    0.703704     0.0  0.127660  0.235294  0.564516   \n",
      "28    0.432099     0.8  0.085106  0.882353  0.489247   \n",
      "29    0.407407     0.8  0.553191  0.882353  0.274194   \n",
      "...        ...     ...       ...       ...       ...   \n",
      "1533  0.777778     0.8  0.382979  0.941176  0.532258   \n",
      "1534  0.555556     1.0  0.063830  1.000000  0.510753   \n",
      "1535  0.654321     1.0  0.148936  0.941176  0.473118   \n",
      "1536  0.703704     0.8  0.085106  0.823529  0.532258   \n",
      "1537  0.518519     0.8  0.297872  0.882353  0.301075   \n",
      "1538  0.469136     0.0  0.127660  0.823529  0.543011   \n",
      "1539  0.629630     0.0  0.468085  0.000000  0.032258   \n",
      "1540  0.679012     0.2  0.191489  0.176471  0.548387   \n",
      "1541  0.703704     0.8  0.234043  0.352941  0.586022   \n",
      "1542  0.740741     0.8  0.574468  0.117647  0.166667   \n",
      "1543  0.691358     0.8  0.148936  0.176471  0.505376   \n",
      "1544  0.728395     0.8  0.425532  0.882353  0.306452   \n",
      "1545  0.000000     0.8  0.446809  0.294118  0.489247   \n",
      "1546  0.000000     0.8  0.170213  0.941176  0.376344   \n",
      "1547  0.666667     0.4  0.170213  0.352941  0.397849   \n",
      "1548  0.666667     0.2  0.106383  0.941176  0.526882   \n",
      "1549  0.691358     0.0  0.723404  0.000000  0.494624   \n",
      "1550  0.703704     0.6  0.085106  0.235294  0.451613   \n",
      "1551  0.666667     0.0  0.489362  0.882353  0.526882   \n",
      "1552  0.777778     0.8  0.170213  0.470588  0.516129   \n",
      "1553  0.000000     0.8  0.276596  0.470588  0.075269   \n",
      "1554  0.679012     0.8  0.361702  0.235294  0.134409   \n",
      "1555  0.814815     0.4  0.446809  0.470588  0.537634   \n",
      "1556  0.654321     0.8  0.765957  0.941176  0.370968   \n",
      "1557  0.654321     0.0  0.425532  0.176471  0.397849   \n",
      "1558  0.506173     0.6  0.148936  0.235294  0.091398   \n",
      "1559  0.641975     0.4  0.574468  0.470588  0.580645   \n",
      "1560  0.000000     0.6  0.276596  0.588235  0.655914   \n",
      "1561  0.000000     0.8  0.255319  0.705882  0.505376   \n",
      "1562  0.679012     0.8  0.510638  0.705882  0.532258   \n",
      "\n",
      "                ...             365天内信用卡查询次数  历史近两年逾期最大天数    ULC33S  近三个月逾期天数  \\\n",
      "0               ...                 0.222222     0.089109  0.053870  0.018018   \n",
      "1               ...                 0.000000     0.000000  0.066993  0.000000   \n",
      "2               ...                 0.111111     0.000000  0.075660  0.000000   \n",
      "3               ...                 0.000000     0.039604  0.009964  0.008008   \n",
      "4               ...                 0.000000     0.000000  0.058846  0.000000   \n",
      "5               ...                 0.055556     0.014851  0.067188  0.000000   \n",
      "6               ...                 0.000000     0.316832  0.060200  0.000000   \n",
      "7               ...                 0.000000     0.000000  0.034518  0.000000   \n",
      "8               ...                 0.000000     0.000000  0.146386  0.000000   \n",
      "9               ...                 0.722222     0.004950  0.017730  0.001001   \n",
      "10              ...                 0.055556     0.000000  0.044134  0.061061   \n",
      "11              ...                 0.055556     0.158416  0.165757  0.032032   \n",
      "12              ...                 0.055556     0.059406  0.013357  0.012012   \n",
      "13              ...                 0.166667     0.009901  0.097403  0.000000   \n",
      "14              ...                 0.611111     0.000000  0.025097  0.000000   \n",
      "15              ...                 0.055556     0.019802  0.044158  0.000000   \n",
      "16              ...                 0.444444     0.000000  0.034405  0.001001   \n",
      "17              ...                 0.055556     0.000000  0.006963  0.000000   \n",
      "18              ...                 0.000000     0.004950  0.035996  0.000000   \n",
      "19              ...                 0.000000     0.000000  0.078938  0.001001   \n",
      "20              ...                 0.000000     0.019802  0.018606  0.004004   \n",
      "21              ...                 0.111111     0.000000  0.038688  0.000000   \n",
      "22              ...                 0.000000     0.000000  0.038920  0.000000   \n",
      "23              ...                 0.000000     0.000000  0.025075  0.000000   \n",
      "24              ...                 0.000000     0.000000  0.034518  0.000000   \n",
      "25              ...                 0.000000     0.000000  0.120708  0.030030   \n",
      "26              ...                 0.388889     0.084158  0.022975  0.004004   \n",
      "27              ...                 0.166667     0.000000  0.018694  0.000000   \n",
      "28              ...                 0.055556     0.000000  0.005785  0.000000   \n",
      "29              ...                 0.166667     0.044554  0.091990  0.009009   \n",
      "...             ...                      ...          ...       ...       ...   \n",
      "1533            ...                 0.000000     0.000000  0.039473  0.000000   \n",
      "1534            ...                 0.222222     0.000000  0.059009  0.000000   \n",
      "1535            ...                 0.055556     0.148515  0.024419  0.001001   \n",
      "1536            ...                 0.111111     0.000000  0.028169  0.000000   \n",
      "1537            ...                 0.000000     0.039604  0.039638  0.004004   \n",
      "1538            ...                 1.000000     0.000000  0.077543  0.001001   \n",
      "1539            ...                 0.000000     0.000000  0.003913  0.000000   \n",
      "1540            ...                 0.000000     0.000000  0.042529  0.000000   \n",
      "1541            ...                 0.055556     0.064356  0.035895  0.013013   \n",
      "1542            ...                 0.055556     0.000000  0.067176  0.000000   \n",
      "1543            ...                 0.000000     0.034653  0.048683  0.007007   \n",
      "1544            ...                 0.000000     0.000000  0.158347  0.000000   \n",
      "1545            ...                 0.000000     0.123762  0.082991  0.030030   \n",
      "1546            ...                 0.000000     0.000000  0.034518  0.000000   \n",
      "1547            ...                 0.000000     0.000000  0.000000  0.000000   \n",
      "1548            ...                 0.000000     0.000000  0.052565  0.000000   \n",
      "1549            ...                 0.000000     0.148515  0.352921  0.001001   \n",
      "1550            ...                 0.000000     0.000000  0.024116  0.005005   \n",
      "1551            ...                 0.055556     0.000000  0.147801  0.000000   \n",
      "1552            ...                 0.000000     0.000000  0.043062  0.000000   \n",
      "1553            ...                 0.000000     0.000000  0.070729  0.000000   \n",
      "1554            ...                 0.166667     0.004950  0.109830  0.001001   \n",
      "1555            ...                 0.055556     0.000000  0.159692  0.000000   \n",
      "1556            ...                 0.000000     0.000000  0.022818  0.000000   \n",
      "1557            ...                 0.000000     0.000000  0.034518  0.012012   \n",
      "1558            ...                 0.666667     0.000000  0.013317  0.001001   \n",
      "1559            ...                 0.000000     0.000000  0.062588  0.000000   \n",
      "1560            ...                 1.000000     0.000000  0.016367  0.001001   \n",
      "1561            ...                 0.111111     0.000000  0.068259  0.001001   \n",
      "1562            ...                 0.000000     0.000000  0.141539  0.000000   \n",
      "\n",
      "         G207O  dsr_before   身份证号首字母  历史申请贷款查询数     开户成功率  \\\n",
      "0     0.086727    0.688679  1.000000   0.206186  0.083333   \n",
      "1     0.062535    0.424528  1.000000   0.061856  0.018889   \n",
      "2     0.136909    0.773585  0.272727   0.309278  0.122222   \n",
      "3     0.015911    0.075472  0.909091   0.278351  0.070000   \n",
      "4     0.075624    0.207547  0.909091   0.041237  0.138889   \n",
      "5     0.073855    0.943396  0.454545   0.288660  0.078889   \n",
      "6     0.101108    0.594340  0.909091   0.494845  0.018889   \n",
      "7     0.046168    0.301887  0.909091   0.216495  0.011111   \n",
      "8     0.077856    0.424528  1.000000   0.092784  0.036667   \n",
      "9     0.028692    0.273585  0.636364   0.319588  0.032222   \n",
      "10    0.042385    0.424528  0.909091   0.123711  0.027778   \n",
      "11    0.196988    0.377358  0.454545   0.391753  0.037778   \n",
      "12    0.033836    0.292453  0.727273   0.206186  0.050000   \n",
      "13    0.170984    0.509434  0.909091   0.185567  0.024444   \n",
      "14    0.022242    0.594340  0.909091   0.154639  0.030000   \n",
      "15    0.064219    0.481132  0.909091   0.268041  0.124444   \n",
      "16    0.046357    0.415094  0.909091   0.257732  0.017778   \n",
      "17    0.017294    0.132075  0.454545   0.061856  0.185556   \n",
      "18    0.041191    0.311321  0.909091   0.144330  0.071111   \n",
      "19    0.085639    0.575472  1.000000   0.051546  0.022222   \n",
      "20    0.036931    0.169811  0.818182   0.123711  0.138889   \n",
      "21    0.070178    0.698113  0.272727   0.474227  0.041111   \n",
      "22    0.047431    0.471698  1.000000   0.298969  0.057778   \n",
      "23    0.038082    0.301887  1.000000   0.144330  0.063333   \n",
      "24    0.046168    0.000000  0.909091   0.072165  0.000000   \n",
      "25    0.123553    0.830189  0.090909   0.113402  0.101111   \n",
      "26    0.043108    0.339623  0.909091   0.164948  0.090000   \n",
      "27    0.025042    0.283019  0.909091   0.010309  0.111111   \n",
      "28    0.006345    0.141509  0.727273   0.061856  0.036667   \n",
      "29    0.105488    0.764151  0.454545   0.082474  0.166667   \n",
      "...        ...         ...       ...        ...       ...   \n",
      "1533  0.035408    0.264151  1.000000   0.020619  0.277778   \n",
      "1534  0.068009    0.764151  0.727273   0.041237  0.138889   \n",
      "1535  0.025393    0.226415  0.909091   0.154639  0.036667   \n",
      "1536  0.028516    0.481132  0.909091   0.092784  0.062222   \n",
      "1537  0.075182    0.726415  0.636364   0.144330  0.103333   \n",
      "1538  0.073097    0.650943  0.818182   0.164948  0.034444   \n",
      "1539  0.004078    0.056604  0.454545   0.000000  0.000000   \n",
      "1540  0.055179    0.245283  0.909091   0.041237  0.083333   \n",
      "1541  0.060324    0.452830  0.909091   0.164948  0.083333   \n",
      "1542  0.070437    0.603774  0.454545   0.041237  0.277778   \n",
      "1543  0.073926    0.500000  0.909091   0.216495  0.095556   \n",
      "1544  0.182565    0.613208  1.000000   0.402062  0.096667   \n",
      "1545  0.282325    0.952830  0.636364   0.175258  0.091111   \n",
      "1546  0.029765    0.198113  0.818182   0.144330  0.032222   \n",
      "1547  0.000000    0.000000  0.818182   0.020619  0.000000   \n",
      "1548  0.057053    0.481132  0.727273   0.000000  0.000000   \n",
      "1549  0.403422    0.707547  0.090909   0.123711  0.046667   \n",
      "1550  0.020796    0.188679  0.909091   0.020619  0.166667   \n",
      "1551  0.114366    0.556604  0.454545   0.195876  0.116667   \n",
      "1552  0.082320    0.688679  0.909091   0.164948  0.083333   \n",
      "1553  0.083885    0.462264  1.000000   0.134021  0.120000   \n",
      "1554  0.237786    0.801887  1.000000   0.453608  0.085556   \n",
      "1555  0.125462    0.424528  1.000000   0.010309  0.111111   \n",
      "1556  0.059320    0.396226  0.272727   0.092784  0.234444   \n",
      "1557  0.046168    0.000000  1.000000   0.144330  0.055556   \n",
      "1558  0.036264    0.330189  0.909091   0.020619  0.166667   \n",
      "1559  0.058682    0.311321  0.454545   0.010309  0.222222   \n",
      "1560  0.014114    0.207547  1.000000   0.092784  0.012222   \n",
      "1561  0.085976    0.575472  0.727273   0.051546  0.022222   \n",
      "1562  0.135225    0.632075  0.727273   0.010309  0.000000   \n",
      "\n",
      "      total_monthly_obligation  \n",
      "0                     0.219663  \n",
      "1                     0.160052  \n",
      "2                     0.340545  \n",
      "3                     0.042387  \n",
      "4                     0.192304  \n",
      "5                     0.185162  \n",
      "6                     0.252313  \n",
      "7                     0.000000  \n",
      "8                     0.197804  \n",
      "9                     0.073878  \n",
      "10                    0.107618  \n",
      "11                    0.488560  \n",
      "12                    0.086554  \n",
      "13                    0.000000  \n",
      "14                    0.057985  \n",
      "15                    0.161435  \n",
      "16                    0.117406  \n",
      "17                    0.049252  \n",
      "18                    0.104678  \n",
      "19                    0.216982  \n",
      "20                    0.094198  \n",
      "21                    0.176100  \n",
      "22                    0.120052  \n",
      "23                    0.097017  \n",
      "24                    0.007540  \n",
      "25                    0.307635  \n",
      "26                    0.109416  \n",
      "27                    0.064903  \n",
      "28                    0.021600  \n",
      "29                    0.206399  \n",
      "...                        ...  \n",
      "1533                  0.093212  \n",
      "1534                  0.170774  \n",
      "1535                  0.065767  \n",
      "1536                  0.073446  \n",
      "1537                  0.188448  \n",
      "1538                  0.000000  \n",
      "1539                  0.013247  \n",
      "1540                  0.139144  \n",
      "1541                  0.000000  \n",
      "1542                  0.179524  \n",
      "1543                  0.185335  \n",
      "1544                  0.000000  \n",
      "1545                  0.701617  \n",
      "1546                  0.079308  \n",
      "1547                  0.000000  \n",
      "1548                  0.146546  \n",
      "1549                  1.000000  \n",
      "1550                  0.057207  \n",
      "1551                  0.284981  \n",
      "1552                  0.206018  \n",
      "1553                  0.212659  \n",
      "1554                  0.589088  \n",
      "1555                  0.315106  \n",
      "1556                  0.152131  \n",
      "1557                  0.003182  \n",
      "1558                  0.095322  \n",
      "1559                  0.000000  \n",
      "1560                  0.037959  \n",
      "1561                  0.217812  \n",
      "1562                  0.000000  \n",
      "\n",
      "[1563 rows x 29 columns]\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       1\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      1\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      1\n",
      "23      1\n",
      "24      1\n",
      "25      1\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "1533    0\n",
      "1534    0\n",
      "1535    0\n",
      "1536    0\n",
      "1537    0\n",
      "1538    0\n",
      "1539    0\n",
      "1540    0\n",
      "1541    0\n",
      "1542    0\n",
      "1543    0\n",
      "1544    0\n",
      "1545    0\n",
      "1546    0\n",
      "1547    0\n",
      "1548    0\n",
      "1549    0\n",
      "1550    0\n",
      "1551    0\n",
      "1552    0\n",
      "1553    0\n",
      "1554    0\n",
      "1555    0\n",
      "1556    0\n",
      "1557    0\n",
      "1558    0\n",
      "1559    0\n",
      "1560    0\n",
      "1561    0\n",
      "1562    0\n",
      "Name: target, Length: 1563, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roy/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X = preprocessing()\n",
    "Y = X['target']\n",
    "X = X.drop(['target'],axis=1)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doMetrics(Y_true,Y_predict,clf_name):\n",
    "    from sklearn.metrics import accuracy_score,recall_score,f1_score,confusion_matrix\n",
    "    print(clf_name,\"_Acc:\",accuracy_score(Y_true,Y_predict))\n",
    "    print(clf_name,\"_recall:\",recall_score(Y_true,Y_predict))\n",
    "    print(clf_name,\"_f1-score:\",f1_score(Y_true,Y_predict))\n",
    "    print(confusion_matrix(Y_true,Y_predict,labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1094, 29)\n",
      "115\n",
      "(469, 29)\n",
      "RandomForestClassifier _Acc: 0.765458422175\n",
      "RandomForestClassifier _recall: 0.58\n",
      "RandomForestClassifier _f1-score: 0.345238095238\n",
      "[[330  89]\n",
      " [ 21  29]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "print(X_train.shape)\n",
    "print(Y_train.sum())\n",
    "print(X_test.shape)\n",
    "# tree = DecisionTreeClassifier(random_state=0, class_weight='balanced', min_weight_fraction_leaf=0.05,splitter='random')\n",
    "# tree.fit(X_train,Y_train)\n",
    "# tree.score(X_test,Y_test)\n",
    "# Y_predict = tree.predict(X_test)\n",
    "# doMetrics(Y_test,Y_predict,\"DecisionTreeClassifier\")\n",
    "# print(tree.feature_importances_)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=1000,n_jobs=4,class_weight='balanced', min_weight_fraction_leaf=0.1)\n",
    "rfc.fit(X_train,Y_train)\n",
    "doMetrics(Y_test,rfc.predict(X_test),\"RandomForestClassifier\")\n",
    "plt.bar(range(len(rfc.feature_importances_)),rfc.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(n_estimators=1000,n_jobs=4,class_weight='balanced', min_weight_fraction_leaf=0.1\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'n_estimators':range(100,1000,50),\n",
    "          'min_samples_leaf':range(1,50,5),\n",
    "          'min_weight_fraction_leaf':[*np.arange(0,0.5,0.01)],\n",
    "          'criterion':['gini','entropy'],\n",
    "          'max_features':range(1,len(X.columns)),\n",
    "          'max_depth':range(1,20,1),\n",
    "         }\n",
    "clf = RandomForestClassifier(class_weight='balanced',n_jobs=-1)\n",
    "gs = GridSearchCV(clf,params,cv=5,n_jobs=-1)\n",
    "gs.fit(X,Y)\n",
    "# clf.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': ['gini', 'entropy'],\n",
       " 'max_depth': range(0, 20),\n",
       " 'max_features': range(1, 29),\n",
       " 'min_samples_leaf': range(1, 50, 5),\n",
       " 'min_weight_fraction_leaf': [0.0,\n",
       "  0.01,\n",
       "  0.02,\n",
       "  0.029999999999999999,\n",
       "  0.040000000000000001,\n",
       "  0.050000000000000003,\n",
       "  0.059999999999999998,\n",
       "  0.070000000000000007,\n",
       "  0.080000000000000002,\n",
       "  0.089999999999999997,\n",
       "  0.10000000000000001,\n",
       "  0.11,\n",
       "  0.12,\n",
       "  0.13,\n",
       "  0.14000000000000001,\n",
       "  0.14999999999999999,\n",
       "  0.16,\n",
       "  0.17000000000000001,\n",
       "  0.17999999999999999,\n",
       "  0.19,\n",
       "  0.20000000000000001,\n",
       "  0.20999999999999999,\n",
       "  0.22,\n",
       "  0.23000000000000001,\n",
       "  0.23999999999999999,\n",
       "  0.25,\n",
       "  0.26000000000000001,\n",
       "  0.27000000000000002,\n",
       "  0.28000000000000003,\n",
       "  0.28999999999999998,\n",
       "  0.29999999999999999,\n",
       "  0.31,\n",
       "  0.32000000000000001,\n",
       "  0.33000000000000002,\n",
       "  0.34000000000000002,\n",
       "  0.35000000000000003,\n",
       "  0.35999999999999999,\n",
       "  0.37,\n",
       "  0.38,\n",
       "  0.39000000000000001,\n",
       "  0.40000000000000002,\n",
       "  0.41000000000000003,\n",
       "  0.41999999999999998,\n",
       "  0.42999999999999999,\n",
       "  0.44,\n",
       "  0.45000000000000001,\n",
       "  0.46000000000000002,\n",
       "  0.47000000000000003,\n",
       "  0.47999999999999998,\n",
       "  0.48999999999999999],\n",
       " 'n_estimators': range(100, 1000, 50),\n",
       " 'splitter': ['best', 'random']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'n_estimators':range(100,1000,50),\n",
    "          'min_samples_leaf':range(1,50,5),\n",
    "          'min_weight_fraction_leaf':[*np.arange(0,0.5,0.01)],\n",
    "          'criterion':['gini','entropy'],\n",
    "          'splitter':['best','random'],\n",
    "          'max_features':range(1,len(X.columns)),\n",
    "          'max_depth':range(0,20,1),\n",
    "         }\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "#对决策树进行画图和输出\n",
    "def plotTree(clf,clf_name,feature_names):\n",
    "    dot_data = export_graphviz(clf,\n",
    "                               out_file=None,\n",
    "                               feature_names=feature_names,\n",
    "                               class_names=[\"Good\",\"Bad\"],\n",
    "                               filled=True,\n",
    "                               rounded=True,\n",
    "                               special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    graph.write_pdf(clf_name + \".pdf\")\n",
    "plotTree(tree,\"tree\",X_train.columns.values)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
