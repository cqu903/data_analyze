{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder,OneHotEncoder,Normalizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "import  matplotlib.pyplot as plt\n",
    "# import os\n",
    "# os.environ[\"PATH\"] += os.pathsep + \"D:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\\"\n",
    "# print(os.environ[\"PATH\"])\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./modeldata1.csv',encoding=\"gbk\")\n",
    "data['target'] = data['是否逾期'].map({\n",
    "    \"是\":1,\n",
    "    \"否\":0\n",
    "})\n",
    "data = data.drop(['是否逾期'],axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['closed_accounts', 'open_accounts', 'enquiry_alert', '人脸相似度', '年龄', '信用卡额度使用率', '信用卡数', '信用卡平均额度', '贷款数', '贷款平均额度', '平均分期金额', '平均期数', '90天内贷款查询次数', '180天内贷款查询次数', '120天内信用卡查询次数', '365天内信用卡查询次数', '历史近两年逾期最大天数', 'ULC33S', '近三个月逾期天数', 'G207O', 'dsr_before', '历史申请贷款查询数', '开户成功率', 'total_monthly_obligation']\n",
      "['申请时段', 'grade', '电话使用时长', '分区', '身份证号首字母']\n"
     ]
    }
   ],
   "source": [
    "import numbers\n",
    "allFeatures = list(data.columns)\n",
    "allFeatures.remove('target')\n",
    "\n",
    "numerical_var = []\n",
    "for col in allFeatures:\n",
    "    if len(set(data[col])) == 1:\n",
    "        print('delete {} from the dataset because it is a constant'.format(col))\n",
    "        del data[col]\n",
    "        allFeatures.remove(col)\n",
    "    else:\n",
    "        uniq_valid_vals = [i for i in data[col] if i == i]\n",
    "        uniq_valid_vals = list(set(uniq_valid_vals))\n",
    "        if len(uniq_valid_vals) >= 6 and isinstance(uniq_valid_vals[0], numbers.Real):\n",
    "            numerical_var.append(col)\n",
    "\n",
    "categorical_var = [i for i in allFeatures if i not in numerical_var]\n",
    "print(numerical_var)\n",
    "print(categorical_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    \n",
    "    #label = data['target']\n",
    "    #data = data.drop(['target'],axis=1)\n",
    "    #特征选择\n",
    "    #考虑相关性，可以去除几个属性，但因为总体的属性不多，暂不处理\n",
    "    #特征处理\n",
    "    for i in range(len(numerical_var)):\n",
    "        data[numerical_var[i]] = MinMaxScaler().fit_transform(data[numerical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "\n",
    "    #处理离散值数值化\n",
    "    for i in range(len(categorical_var)):\n",
    "        if categorical_var[i] == \"申请时段\":\n",
    "            data[categorical_var[i]] = [map_apply(s) for s in data[\"申请时段\"].values]\n",
    "            data[categorical_var[i]] = MinMaxScaler().fit_transform(data[categorical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        elif categorical_var[i] == \"grade\":\n",
    "            data[categorical_var[i]] = [map_grade(w) for w in data['grade'].values]\n",
    "            data[categorical_var[i]] = MinMaxScaler().fit_transform(data[categorical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "        else:\n",
    "            data[categorical_var[i]] = LabelEncoder().fit_transform(data[categorical_var[i]])\n",
    "            #对于labelEncoder的情况下还需要进行归一化处理\n",
    "            data[categorical_var[i]] = MinMaxScaler().fit_transform(data[categorical_var[i]].values.reshape(-1,1)).reshape(1,-1)[0]\n",
    "\n",
    "    return data\n",
    "\n",
    "def map_apply(s):\n",
    "    d = dict([(\"0点\",24),(\"1点\",1),(\"2点\",2),(\"3点\",3),(\"4点\",4),(\"5点\",5),(\"6点\",6),(\"7点\",7),(\"8点\",8),(\"9点\",9),(\"10点\",10),(\"11点\",11),\n",
    "             (\"12点\",12),(\"13点\",13),(\"14点\",14),(\"15点\",15),(\"16点\",16),(\"17点\",17),(\"18点\",18),(\"19点\",19),(\"20点\",20),(\"21点\",21),\n",
    "             (\"22点\",22),(\"23点\",23)])\n",
    "    return d.get(s,0)\n",
    "def map_grade(w):\n",
    "    d = dict([(\"AA\",9),(\"BB\",8),(\"CC\",7),(\"DD\",6),(\"EE\",5),(\"FF\",4),(\"GG\",3),(\"HH\",2),(\"II\",1)])\n",
    "    return d.get(w,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          申请时段  grade  closed_accounts  open_accounts  enquiry_alert  \\\n",
      "0     0.869565  0.125         0.325581       0.294118       0.248908   \n",
      "1     0.913043  0.125         0.023256       0.294118       0.275109   \n",
      "2     0.521739  0.000         0.674419       0.352941       0.310044   \n",
      "3     0.434783  0.000         0.465116       0.235294       0.253275   \n",
      "4     0.391304  0.125         0.046512       0.294118       0.196507   \n",
      "5     0.434783  0.000         0.302326       0.705882       0.327511   \n",
      "6     0.521739  0.000         0.162791       0.352941       0.401747   \n",
      "7     0.565217  0.375         0.046512       0.000000       0.096070   \n",
      "8     0.869565  0.125         0.255814       0.411765       0.419214   \n",
      "9     1.000000  0.000         0.116279       0.235294       0.266376   \n",
      "10    0.434783  0.000         0.069767       0.352941       0.327511   \n",
      "11    0.565217  0.000         0.162791       0.470588       0.358079   \n",
      "12    0.608696  0.000         0.069767       0.411765       0.205240   \n",
      "13    0.608696  0.000         0.023256       0.470588       0.235808   \n",
      "14    0.695652  0.000         0.046512       0.294118       0.170306   \n",
      "15    0.739130  0.000         0.604651       0.235294       0.222707   \n",
      "16    0.347826  0.000         0.093023       0.529412       0.510917   \n",
      "17    0.478261  0.375         0.232558       0.117647       0.048035   \n",
      "18    0.695652  0.000         0.116279       0.235294       0.135371   \n",
      "19    0.304348  0.125         0.023256       0.411765       0.161572   \n",
      "20    0.434783  0.000         0.302326       0.294118       0.235808   \n",
      "21    0.434783  0.000         0.232558       0.705882       0.537118   \n",
      "22    0.478261  0.000         0.325581       0.352941       0.296943   \n",
      "23    0.478261  0.000         0.093023       0.235294       0.266376   \n",
      "24    0.478261  0.500         0.000000       0.000000       0.034934   \n",
      "25    0.565217  0.000         0.232558       0.411765       0.423581   \n",
      "26    0.565217  0.000         0.348837       0.529412       0.157205   \n",
      "27    0.608696  0.000         0.046512       0.294118       0.043668   \n",
      "28    0.782609  0.125         0.093023       0.117647       0.048035   \n",
      "29    0.739130  0.000         0.255814       0.411765       0.275109   \n",
      "...        ...    ...              ...            ...            ...   \n",
      "1533  0.956522  0.125         0.093023       0.117647       0.279476   \n",
      "1534  0.000000  0.000         0.023256       0.411765       0.231441   \n",
      "1535  0.521739  0.000         0.023256       0.294118       0.296943   \n",
      "1536  0.739130  0.000         0.046512       0.411765       0.183406   \n",
      "1537  0.782609  0.000         0.209302       0.470588       0.537118   \n",
      "1538  0.782609  0.000         0.046512       0.470588       0.471616   \n",
      "1539  0.130435  0.875         0.209302       0.705882       0.454148   \n",
      "1540  1.000000  0.000         0.023256       0.235294       0.222707   \n",
      "1541  0.347826  0.000         0.302326       0.294118       0.432314   \n",
      "1542  0.913043  0.250         0.232558       0.176471       0.218341   \n",
      "1543  0.521739  0.000         0.279070       0.352941       0.305677   \n",
      "1544  0.565217  0.000         0.651163       0.470588       0.502183   \n",
      "1545  0.652174  0.125         0.255814       0.470588       0.323144   \n",
      "1546  1.000000  0.250         0.093023       0.117647       0.344978   \n",
      "1547  0.173913  0.875         0.046512       0.117647       0.039301   \n",
      "1548  0.782609  0.250         0.000000       0.176471       0.349345   \n",
      "1549  0.434783  0.125         0.418605       0.823529       0.606987   \n",
      "1550  0.478261  0.250         0.046512       0.117647       0.209607   \n",
      "1551  0.652174  0.000         0.395349       0.235294       0.244541   \n",
      "1552  0.304348  0.000         0.232558       0.235294       0.209607   \n",
      "1553  0.347826  0.125         0.348837       0.235294       0.358079   \n",
      "1554  0.478261  0.000         0.604651       0.705882       0.545852   \n",
      "1555  0.565217  0.125         0.116279       0.352941       0.144105   \n",
      "1556  0.608696  0.125         0.325581       0.352941       0.344978   \n",
      "1557  0.652174  0.000         0.255814       0.000000       0.157205   \n",
      "1558  0.478261  0.250         0.093023       0.352941       0.244541   \n",
      "1559  0.391304  0.250         0.000000       0.529412       0.248908   \n",
      "1560  0.217391  0.000         0.186047       0.235294       0.187773   \n",
      "1561  0.695652  0.125         0.046512       0.294118       0.558952   \n",
      "1562  0.869565  0.375         0.000000       0.529412       0.314410   \n",
      "\n",
      "         人脸相似度  电话使用时长        年龄        分区  信用卡额度使用率  \\\n",
      "0     0.419753     0.0  0.361702  0.176471  0.526882   \n",
      "1     0.679012     0.8  0.319149  0.823529  0.543011   \n",
      "2     0.666667     0.8  0.595745  0.117647  0.349462   \n",
      "3     0.691358     0.8  0.191489  0.411765  0.000000   \n",
      "4     0.617284     0.8  0.212766  0.411765  0.526882   \n",
      "5     0.839506     0.8  0.574468  0.882353  0.317204   \n",
      "6     0.580247     1.0  0.170213  0.941176  0.000000   \n",
      "7     0.938272     0.2  0.170213  0.941176  0.397849   \n",
      "8     0.740741     0.0  0.319149  0.235294  0.252688   \n",
      "9     0.703704     0.2  0.425532  0.235294  0.532258   \n",
      "10    0.444444     0.8  0.170213  1.000000  0.236559   \n",
      "11    0.481481     0.8  0.489362  0.529412  0.494624   \n",
      "12    0.790123     0.0  0.085106  0.823529  0.204301   \n",
      "13    0.382716     1.0  0.170213  0.823529  0.317204   \n",
      "14    0.790123     1.0  0.042553  0.882353  0.553763   \n",
      "15    0.567901     0.8  0.148936  0.529412  0.451613   \n",
      "16    0.790123     0.0  0.127660  0.411765  0.413978   \n",
      "17    0.666667     0.8  0.595745  0.117647  0.537634   \n",
      "18    0.543210     1.0  0.148936  0.176471  0.532258   \n",
      "19    0.407407     0.4  0.276596  0.294118  0.537634   \n",
      "20    0.777778     0.0  0.234043  0.823529  0.440860   \n",
      "21    0.753086     0.0  0.638298  0.411765  0.220430   \n",
      "22    0.654321     0.8  0.319149  1.000000  0.510753   \n",
      "23    0.753086     0.8  0.255319  0.470588  0.440860   \n",
      "24    0.876543     0.0  0.042553  0.117647  0.397849   \n",
      "25    0.493827     0.8  0.638298  0.294118  0.532258   \n",
      "26    0.469136     1.0  0.148936  0.411765  0.446237   \n",
      "27    0.703704     0.0  0.127660  0.235294  0.564516   \n",
      "28    0.432099     0.8  0.085106  0.882353  0.489247   \n",
      "29    0.407407     0.8  0.553191  0.882353  0.274194   \n",
      "...        ...     ...       ...       ...       ...   \n",
      "1533  0.777778     0.8  0.382979  0.941176  0.532258   \n",
      "1534  0.555556     1.0  0.063830  1.000000  0.510753   \n",
      "1535  0.654321     1.0  0.148936  0.941176  0.473118   \n",
      "1536  0.703704     0.8  0.085106  0.823529  0.532258   \n",
      "1537  0.518519     0.8  0.297872  0.882353  0.301075   \n",
      "1538  0.469136     0.0  0.127660  0.823529  0.543011   \n",
      "1539  0.629630     0.0  0.468085  0.000000  0.032258   \n",
      "1540  0.679012     0.2  0.191489  0.176471  0.548387   \n",
      "1541  0.703704     0.8  0.234043  0.352941  0.586022   \n",
      "1542  0.740741     0.8  0.574468  0.117647  0.166667   \n",
      "1543  0.691358     0.8  0.148936  0.176471  0.505376   \n",
      "1544  0.728395     0.8  0.425532  0.882353  0.306452   \n",
      "1545  0.000000     0.8  0.446809  0.294118  0.489247   \n",
      "1546  0.000000     0.8  0.170213  0.941176  0.376344   \n",
      "1547  0.666667     0.4  0.170213  0.352941  0.397849   \n",
      "1548  0.666667     0.2  0.106383  0.941176  0.526882   \n",
      "1549  0.691358     0.0  0.723404  0.000000  0.494624   \n",
      "1550  0.703704     0.6  0.085106  0.235294  0.451613   \n",
      "1551  0.666667     0.0  0.489362  0.882353  0.526882   \n",
      "1552  0.777778     0.8  0.170213  0.470588  0.516129   \n",
      "1553  0.000000     0.8  0.276596  0.470588  0.075269   \n",
      "1554  0.679012     0.8  0.361702  0.235294  0.134409   \n",
      "1555  0.814815     0.4  0.446809  0.470588  0.537634   \n",
      "1556  0.654321     0.8  0.765957  0.941176  0.370968   \n",
      "1557  0.654321     0.0  0.425532  0.176471  0.397849   \n",
      "1558  0.506173     0.6  0.148936  0.235294  0.091398   \n",
      "1559  0.641975     0.4  0.574468  0.470588  0.580645   \n",
      "1560  0.000000     0.6  0.276596  0.588235  0.655914   \n",
      "1561  0.000000     0.8  0.255319  0.705882  0.505376   \n",
      "1562  0.679012     0.8  0.510638  0.705882  0.532258   \n",
      "\n",
      "                ...             365天内信用卡查询次数  历史近两年逾期最大天数    ULC33S  近三个月逾期天数  \\\n",
      "0               ...                 0.222222     0.089109  0.053870  0.018018   \n",
      "1               ...                 0.000000     0.000000  0.066993  0.000000   \n",
      "2               ...                 0.111111     0.000000  0.075660  0.000000   \n",
      "3               ...                 0.000000     0.039604  0.009964  0.008008   \n",
      "4               ...                 0.000000     0.000000  0.058846  0.000000   \n",
      "5               ...                 0.055556     0.014851  0.067188  0.000000   \n",
      "6               ...                 0.000000     0.316832  0.060200  0.000000   \n",
      "7               ...                 0.000000     0.000000  0.034518  0.000000   \n",
      "8               ...                 0.000000     0.000000  0.146386  0.000000   \n",
      "9               ...                 0.722222     0.004950  0.017730  0.001001   \n",
      "10              ...                 0.055556     0.000000  0.044134  0.061061   \n",
      "11              ...                 0.055556     0.158416  0.165757  0.032032   \n",
      "12              ...                 0.055556     0.059406  0.013357  0.012012   \n",
      "13              ...                 0.166667     0.009901  0.097403  0.000000   \n",
      "14              ...                 0.611111     0.000000  0.025097  0.000000   \n",
      "15              ...                 0.055556     0.019802  0.044158  0.000000   \n",
      "16              ...                 0.444444     0.000000  0.034405  0.001001   \n",
      "17              ...                 0.055556     0.000000  0.006963  0.000000   \n",
      "18              ...                 0.000000     0.004950  0.035996  0.000000   \n",
      "19              ...                 0.000000     0.000000  0.078938  0.001001   \n",
      "20              ...                 0.000000     0.019802  0.018606  0.004004   \n",
      "21              ...                 0.111111     0.000000  0.038688  0.000000   \n",
      "22              ...                 0.000000     0.000000  0.038920  0.000000   \n",
      "23              ...                 0.000000     0.000000  0.025075  0.000000   \n",
      "24              ...                 0.000000     0.000000  0.034518  0.000000   \n",
      "25              ...                 0.000000     0.000000  0.120708  0.030030   \n",
      "26              ...                 0.388889     0.084158  0.022975  0.004004   \n",
      "27              ...                 0.166667     0.000000  0.018694  0.000000   \n",
      "28              ...                 0.055556     0.000000  0.005785  0.000000   \n",
      "29              ...                 0.166667     0.044554  0.091990  0.009009   \n",
      "...             ...                      ...          ...       ...       ...   \n",
      "1533            ...                 0.000000     0.000000  0.039473  0.000000   \n",
      "1534            ...                 0.222222     0.000000  0.059009  0.000000   \n",
      "1535            ...                 0.055556     0.148515  0.024419  0.001001   \n",
      "1536            ...                 0.111111     0.000000  0.028169  0.000000   \n",
      "1537            ...                 0.000000     0.039604  0.039638  0.004004   \n",
      "1538            ...                 1.000000     0.000000  0.077543  0.001001   \n",
      "1539            ...                 0.000000     0.000000  0.003913  0.000000   \n",
      "1540            ...                 0.000000     0.000000  0.042529  0.000000   \n",
      "1541            ...                 0.055556     0.064356  0.035895  0.013013   \n",
      "1542            ...                 0.055556     0.000000  0.067176  0.000000   \n",
      "1543            ...                 0.000000     0.034653  0.048683  0.007007   \n",
      "1544            ...                 0.000000     0.000000  0.158347  0.000000   \n",
      "1545            ...                 0.000000     0.123762  0.082991  0.030030   \n",
      "1546            ...                 0.000000     0.000000  0.034518  0.000000   \n",
      "1547            ...                 0.000000     0.000000  0.000000  0.000000   \n",
      "1548            ...                 0.000000     0.000000  0.052565  0.000000   \n",
      "1549            ...                 0.000000     0.148515  0.352921  0.001001   \n",
      "1550            ...                 0.000000     0.000000  0.024116  0.005005   \n",
      "1551            ...                 0.055556     0.000000  0.147801  0.000000   \n",
      "1552            ...                 0.000000     0.000000  0.043062  0.000000   \n",
      "1553            ...                 0.000000     0.000000  0.070729  0.000000   \n",
      "1554            ...                 0.166667     0.004950  0.109830  0.001001   \n",
      "1555            ...                 0.055556     0.000000  0.159692  0.000000   \n",
      "1556            ...                 0.000000     0.000000  0.022818  0.000000   \n",
      "1557            ...                 0.000000     0.000000  0.034518  0.012012   \n",
      "1558            ...                 0.666667     0.000000  0.013317  0.001001   \n",
      "1559            ...                 0.000000     0.000000  0.062588  0.000000   \n",
      "1560            ...                 1.000000     0.000000  0.016367  0.001001   \n",
      "1561            ...                 0.111111     0.000000  0.068259  0.001001   \n",
      "1562            ...                 0.000000     0.000000  0.141539  0.000000   \n",
      "\n",
      "         G207O  dsr_before   身份证号首字母  历史申请贷款查询数     开户成功率  \\\n",
      "0     0.086727    0.688679  1.000000   0.206186  0.083333   \n",
      "1     0.062535    0.424528  1.000000   0.061856  0.018889   \n",
      "2     0.136909    0.773585  0.272727   0.309278  0.122222   \n",
      "3     0.015911    0.075472  0.909091   0.278351  0.070000   \n",
      "4     0.075624    0.207547  0.909091   0.041237  0.138889   \n",
      "5     0.073855    0.943396  0.454545   0.288660  0.078889   \n",
      "6     0.101108    0.594340  0.909091   0.494845  0.018889   \n",
      "7     0.046168    0.301887  0.909091   0.216495  0.011111   \n",
      "8     0.077856    0.424528  1.000000   0.092784  0.036667   \n",
      "9     0.028692    0.273585  0.636364   0.319588  0.032222   \n",
      "10    0.042385    0.424528  0.909091   0.123711  0.027778   \n",
      "11    0.196988    0.377358  0.454545   0.391753  0.037778   \n",
      "12    0.033836    0.292453  0.727273   0.206186  0.050000   \n",
      "13    0.170984    0.509434  0.909091   0.185567  0.024444   \n",
      "14    0.022242    0.594340  0.909091   0.154639  0.030000   \n",
      "15    0.064219    0.481132  0.909091   0.268041  0.124444   \n",
      "16    0.046357    0.415094  0.909091   0.257732  0.017778   \n",
      "17    0.017294    0.132075  0.454545   0.061856  0.185556   \n",
      "18    0.041191    0.311321  0.909091   0.144330  0.071111   \n",
      "19    0.085639    0.575472  1.000000   0.051546  0.022222   \n",
      "20    0.036931    0.169811  0.818182   0.123711  0.138889   \n",
      "21    0.070178    0.698113  0.272727   0.474227  0.041111   \n",
      "22    0.047431    0.471698  1.000000   0.298969  0.057778   \n",
      "23    0.038082    0.301887  1.000000   0.144330  0.063333   \n",
      "24    0.046168    0.000000  0.909091   0.072165  0.000000   \n",
      "25    0.123553    0.830189  0.090909   0.113402  0.101111   \n",
      "26    0.043108    0.339623  0.909091   0.164948  0.090000   \n",
      "27    0.025042    0.283019  0.909091   0.010309  0.111111   \n",
      "28    0.006345    0.141509  0.727273   0.061856  0.036667   \n",
      "29    0.105488    0.764151  0.454545   0.082474  0.166667   \n",
      "...        ...         ...       ...        ...       ...   \n",
      "1533  0.035408    0.264151  1.000000   0.020619  0.277778   \n",
      "1534  0.068009    0.764151  0.727273   0.041237  0.138889   \n",
      "1535  0.025393    0.226415  0.909091   0.154639  0.036667   \n",
      "1536  0.028516    0.481132  0.909091   0.092784  0.062222   \n",
      "1537  0.075182    0.726415  0.636364   0.144330  0.103333   \n",
      "1538  0.073097    0.650943  0.818182   0.164948  0.034444   \n",
      "1539  0.004078    0.056604  0.454545   0.000000  0.000000   \n",
      "1540  0.055179    0.245283  0.909091   0.041237  0.083333   \n",
      "1541  0.060324    0.452830  0.909091   0.164948  0.083333   \n",
      "1542  0.070437    0.603774  0.454545   0.041237  0.277778   \n",
      "1543  0.073926    0.500000  0.909091   0.216495  0.095556   \n",
      "1544  0.182565    0.613208  1.000000   0.402062  0.096667   \n",
      "1545  0.282325    0.952830  0.636364   0.175258  0.091111   \n",
      "1546  0.029765    0.198113  0.818182   0.144330  0.032222   \n",
      "1547  0.000000    0.000000  0.818182   0.020619  0.000000   \n",
      "1548  0.057053    0.481132  0.727273   0.000000  0.000000   \n",
      "1549  0.403422    0.707547  0.090909   0.123711  0.046667   \n",
      "1550  0.020796    0.188679  0.909091   0.020619  0.166667   \n",
      "1551  0.114366    0.556604  0.454545   0.195876  0.116667   \n",
      "1552  0.082320    0.688679  0.909091   0.164948  0.083333   \n",
      "1553  0.083885    0.462264  1.000000   0.134021  0.120000   \n",
      "1554  0.237786    0.801887  1.000000   0.453608  0.085556   \n",
      "1555  0.125462    0.424528  1.000000   0.010309  0.111111   \n",
      "1556  0.059320    0.396226  0.272727   0.092784  0.234444   \n",
      "1557  0.046168    0.000000  1.000000   0.144330  0.055556   \n",
      "1558  0.036264    0.330189  0.909091   0.020619  0.166667   \n",
      "1559  0.058682    0.311321  0.454545   0.010309  0.222222   \n",
      "1560  0.014114    0.207547  1.000000   0.092784  0.012222   \n",
      "1561  0.085976    0.575472  0.727273   0.051546  0.022222   \n",
      "1562  0.135225    0.632075  0.727273   0.010309  0.000000   \n",
      "\n",
      "      total_monthly_obligation  \n",
      "0                     0.219663  \n",
      "1                     0.160052  \n",
      "2                     0.340545  \n",
      "3                     0.042387  \n",
      "4                     0.192304  \n",
      "5                     0.185162  \n",
      "6                     0.252313  \n",
      "7                     0.000000  \n",
      "8                     0.197804  \n",
      "9                     0.073878  \n",
      "10                    0.107618  \n",
      "11                    0.488560  \n",
      "12                    0.086554  \n",
      "13                    0.000000  \n",
      "14                    0.057985  \n",
      "15                    0.161435  \n",
      "16                    0.117406  \n",
      "17                    0.049252  \n",
      "18                    0.104678  \n",
      "19                    0.216982  \n",
      "20                    0.094198  \n",
      "21                    0.176100  \n",
      "22                    0.120052  \n",
      "23                    0.097017  \n",
      "24                    0.007540  \n",
      "25                    0.307635  \n",
      "26                    0.109416  \n",
      "27                    0.064903  \n",
      "28                    0.021600  \n",
      "29                    0.206399  \n",
      "...                        ...  \n",
      "1533                  0.093212  \n",
      "1534                  0.170774  \n",
      "1535                  0.065767  \n",
      "1536                  0.073446  \n",
      "1537                  0.188448  \n",
      "1538                  0.000000  \n",
      "1539                  0.013247  \n",
      "1540                  0.139144  \n",
      "1541                  0.000000  \n",
      "1542                  0.179524  \n",
      "1543                  0.185335  \n",
      "1544                  0.000000  \n",
      "1545                  0.701617  \n",
      "1546                  0.079308  \n",
      "1547                  0.000000  \n",
      "1548                  0.146546  \n",
      "1549                  1.000000  \n",
      "1550                  0.057207  \n",
      "1551                  0.284981  \n",
      "1552                  0.206018  \n",
      "1553                  0.212659  \n",
      "1554                  0.589088  \n",
      "1555                  0.315106  \n",
      "1556                  0.152131  \n",
      "1557                  0.003182  \n",
      "1558                  0.095322  \n",
      "1559                  0.000000  \n",
      "1560                  0.037959  \n",
      "1561                  0.217812  \n",
      "1562                  0.000000  \n",
      "\n",
      "[1563 rows x 29 columns]\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       1\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      1\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      1\n",
      "23      1\n",
      "24      1\n",
      "25      1\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "1533    0\n",
      "1534    0\n",
      "1535    0\n",
      "1536    0\n",
      "1537    0\n",
      "1538    0\n",
      "1539    0\n",
      "1540    0\n",
      "1541    0\n",
      "1542    0\n",
      "1543    0\n",
      "1544    0\n",
      "1545    0\n",
      "1546    0\n",
      "1547    0\n",
      "1548    0\n",
      "1549    0\n",
      "1550    0\n",
      "1551    0\n",
      "1552    0\n",
      "1553    0\n",
      "1554    0\n",
      "1555    0\n",
      "1556    0\n",
      "1557    0\n",
      "1558    0\n",
      "1559    0\n",
      "1560    0\n",
      "1561    0\n",
      "1562    0\n",
      "Name: target, Length: 1563, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roy/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "X = preprocessing()\n",
    "Y = X['target']\n",
    "X = X.drop(['target'],axis=1)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,f1_score,confusion_matrix,precision_score\n",
    "def doMetrics(Y_true,Y_predict,clf_name):\n",
    "    print(clf_name,\"_Acc:\",accuracy_score(Y_true,Y_predict))\n",
    "    print(clf_name,\"_precision_socre:\",precision_score(Y_true,Y_predict))\n",
    "    print(clf_name,\"_recall:\",recall_score(Y_true,Y_predict))\n",
    "    print(clf_name,\"_f1-score:\",f1_score(Y_true,Y_predict))\n",
    "    print(confusion_matrix(Y_true,Y_predict,labels=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1094, 29)\n",
      "115\n",
      "(469, 29)\n",
      "RandomForestClassifier _Acc: 0.769722814499\n",
      "RandomForestClassifier _precision_socre: 0.241071428571\n",
      "RandomForestClassifier _recall: 0.54\n",
      "RandomForestClassifier _f1-score: 0.333333333333\n",
      "[[334  85]\n",
      " [ 23  27]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAETBJREFUeJzt3X+s3Xddx/Hny5YNHTpgXAzuhy2u\nmnRCEEoxEZFAgE7iinGFjkQ6M1NMqNFgIsXoGBWSjSDDhGmobmZs4phDtMmqlTCjhozZu4Eb3Rxc\nZmWXEtbRORxkjG5v/zjfxuPhtvd77z3r7T2f5yNp+v1+vp/vue9Pvrmv8+nnfM+3qSokSW34geUu\nQJJ08hj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIasXu4CRj3vec+rNWvWLHcZ\nkrSi3HnnnQ9X1dR8/U650F+zZg3T09PLXYYkrShJ/qtPP5d3JKkhhr4kNcTQl6SGGPqS1BBDX5Ia\nYuhLUkMMfUlqiKEvSQ0x9CWpIafcN3KlSbdm560nPH7wyjeepErUImf6ktQQQ1+SGmLoS1JDDH1J\naoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDekV+kk2Jbk/yUySnXMc\nf1WSu5IcTXLxUPtLktye5ECSu5O8ZZzFS5IWZt7QT7IKuAa4EFgPXJJk/Ui3rwKXAh8faf8O8Laq\nugDYBHw4ybOXWrQkaXH6PE9/IzBTVQ8AJLkJ2Azce6xDVR3sjj01fGJVfWlo+1CSh4Ap4L+XXLkk\nacH6LO+cDTw4tD/btS1Iko3AacBX5ji2Pcl0kunDhw8v9KUlST31Cf3M0VYL+SFJXgDcAPxaVT01\neryqdlfVhqraMDU1tZCXliQtQJ/QnwXOHdo/BzjU9wck+RHgVuD3q+pzCytPkjROfUJ/P7Auydok\npwFbgT19Xrzr/yngY1X114svU5I0DvOGflUdBXYA+4D7gJur6kCSXUkuAkjy8iSzwBbgo0kOdKe/\nGXgVcGmSL3R/XvK0jESSNK8+d+9QVXuBvSNtlw9t72ew7DN63o3AjUusUZI0Jn4jV5IaYuhLUkMM\nfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCX\npIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNaRX6CfZlOT+JDNJds5x/FVJ7kpyNMnF\nI8e2Jfly92fbuAqXJC3cvKGfZBVwDXAhsB64JMn6kW5fBS4FPj5y7nOB9wCvADYC70nynKWXLUla\njD4z/Y3ATFU9UFVPADcBm4c7VNXBqrobeGrk3DcAn66qI1X1CPBpYNMY6pYkLUKf0D8beHBof7Zr\n62Mp50qSxqxP6GeOtur5+r3OTbI9yXSS6cOHD/d8aUnSQvUJ/Vng3KH9c4BDPV+/17lVtbuqNlTV\nhqmpqZ4vLUlaqD6hvx9Yl2RtktOArcCenq+/D3h9kud0H+C+vmuTJC2DeUO/qo4COxiE9X3AzVV1\nIMmuJBcBJHl5kllgC/DRJAe6c48Af8jgjWM/sKtrkyQtg9V9OlXVXmDvSNvlQ9v7GSzdzHXudcB1\nS6hRkjQmfiNXkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlq\niKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIb0\nCv0km5Lcn2Qmyc45jp+e5BPd8TuSrOnan5Hk+iT3JLkvybvHW74kaSHmDf0kq4BrgAuB9cAlSdaP\ndLsMeKSqzgeuBq7q2rcAp1fVi4CXAW8/9oYgSTr5+sz0NwIzVfVAVT0B3ARsHumzGbi+274FeG2S\nAAWckWQ18IPAE8C3xlK5JGnB+oT+2cCDQ/uzXducfarqKPAocBaDN4BvA18Hvgp8sKqOLLFmSdIi\n9Qn9zNFWPftsBJ4EfgxYC/xOkhd+3w9ItieZTjJ9+PDhHiVJkhajT+jPAucO7Z8DHDpen24p50zg\nCPBW4B+q6ntV9RDwWWDD6A+oqt1VtaGqNkxNTS18FJKkXvqE/n5gXZK1SU4DtgJ7RvrsAbZ12xcD\nt1VVMVjSeU0GzgB+FviP8ZQuSVqoeUO/W6PfAewD7gNurqoDSXYluajrdi1wVpIZ4J3Asds6rwGe\nBXyRwZvHX1TV3WMegySpp9V9OlXVXmDvSNvlQ9uPM7g9c/S8x+ZqlyQtD7+RK0kNMfQlqSGGviQ1\npNeavqSTb83OW+ftc/DKN56ESjRJnOlLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+S\nGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWpIr9BPsinJ/Ulm\nkuyc4/jpST7RHb8jyZqhYy9OcnuSA0nuSfLM8ZUvSVqIeUM/ySrgGuBCYD1wSZL1I90uAx6pqvOB\nq4GrunNXAzcCv1FVFwCvBr43tuolSQvSZ6a/EZipqgeq6gngJmDzSJ/NwPXd9i3Aa5MEeD1wd1X9\nO0BVfbOqnhxP6ZKkheoT+mcDDw7tz3Ztc/apqqPAo8BZwE8ClWRfkruS/O7SS5YkLdbqHn0yR1v1\n7LMaeCXwcuA7wGeS3FlVn/l/Jyfbge0A5513Xo+SJEmL0WemPwucO7R/DnDoeH26dfwzgSNd+z9X\n1cNV9R1gL/DS0R9QVburakNVbZiamlr4KCRJvfQJ/f3AuiRrk5wGbAX2jPTZA2zrti8GbquqAvYB\nL07yQ92bwS8A946ndEnSQs27vFNVR5PsYBDgq4DrqupAkl3AdFXtAa4Fbkgyw2CGv7U795EkH2Lw\nxlHA3qq69WkaiyRpHn3W9KmqvQyWZobbLh/afhzYcpxzb2Rw26YkaZn5jVxJaoihL0kNMfQlqSGG\nviQ1pNcHuZLUkjU757/J8OCVbzwJlYyfM31JaogzfUkr2iTPyp8OzvQlqSGGviQ1xNCXpIYY+pLU\nEENfkhpi6EtSQwx9SWqIoS9JDTH0JakhfiNXGgO/FaqVwpm+JDXE0Jekhhj6ktQQ1/TVHNff1TJn\n+pLUEENfkhrSa3knySbgj4FVwJ9X1ZUjx08HPga8DPgm8JaqOjh0/DzgXuCKqvrgeEpfGv+JL2kc\n5suSUy1H5g39JKuAa4DXAbPA/iR7qureoW6XAY9U1flJtgJXAW8ZOn418PfjK/v4VtoFkKSTqc9M\nfyMwU1UPACS5CdjMYOZ+zGbgim77FuAjSVJVleRNwAPAt8dW9QTwzUnScuizpn828ODQ/mzXNmef\nqjoKPAqcleQM4F3Ae5deqiRpqfqEfuZoq5593gtcXVWPnfAHJNuTTCeZPnz4cI+SJEmL0Wd5ZxY4\nd2j/HODQcfrMJlkNnAkcAV4BXJzkA8CzgaeSPF5VHxk+uap2A7sBNmzYMPqGIkkakz6hvx9Yl2Qt\n8DVgK/DWkT57gG3A7cDFwG1VVcDPH+uQ5ArgsdHAlySdPPOGflUdTbID2Mfgls3rqupAkl3AdFXt\nAa4Fbkgyw2CGv/XpLFqStDi97tOvqr3A3pG2y4e2Hwe2zPMaVyyiPknSGPnsnUZ5y6jUJh/DIEkN\nMfQlqSEu70iak0uAk8nQl3RK8k3n6WHo65TmL740Xq7pS1JDDH1JaoihL0kNMfQlqSF+kCudgB8k\na9IY+pJ0kpwKkwhDv4dT4UJJpzJ/R1YO1/QlqSGGviQ1xNCXpIYY+pLUEENfkhri3Ts66ea70wO8\n22NSee2XnzN9SWqIoS9JDTH0Jakhhr4kNaRX6CfZlOT+JDNJds5x/PQkn+iO35FkTdf+uiR3Jrmn\n+/s14y1fkrQQ84Z+klXANcCFwHrgkiTrR7pdBjxSVecDVwNXde0PA79UVS8CtgE3jKtwSdLC9bll\ncyMwU1UPACS5CdgM3DvUZzNwRbd9C/CRJKmqzw/1OQA8M8npVfXdJVeu77Pct8P50C3p1Ndneeds\n4MGh/dmubc4+VXUUeBQ4a6TPrwCfnyvwk2xPMp1k+vDhw31rlyQtUJ/QzxxttZA+SS5gsOTz9rl+\nQFXtrqoNVbVhamqqR0mSpMXoE/qzwLlD++cAh47XJ8lq4EzgSLd/DvAp4G1V9ZWlFixJWrw+a/r7\ngXVJ1gJfA7YCbx3ps4fBB7W3AxcDt1VVJXk2cCvw7qr67PjKbotr5ZLGZd6ZfrdGvwPYB9wH3FxV\nB5LsSnJR1+1a4KwkM8A7gWO3de4Azgf+IMkXuj/PH/soJEm99HrgWlXtBfaOtF0+tP04sGWO894H\nvG+JNa4Yy333jCTNx2/kSlJDDH1JaoihL0kN8T9R0Qn5OYU0WZzpS1JDnOlrIvgvEqkfZ/qS1BBD\nX5IaYuhLUkMMfUlqiKEvSQ3x7h2pId7lJGf6ktQQQ1+SGmLoS1JDXNOX1Az/Fzpn+pLUFENfkhpi\n6EtSQwx9SWqIoS9JDTH0JakhvUI/yaYk9yeZSbJzjuOnJ/lEd/yOJGuGjr27a78/yRvGV7okaaHm\nDf0kq4BrgAuB9cAlSdaPdLsMeKSqzgeuBq7qzl0PbAUuADYBf9K9niRpGfSZ6W8EZqrqgap6ArgJ\n2DzSZzNwfbd9C/DaJOnab6qq71bVfwIz3etJkpZBn2/kng08OLQ/C7zieH2q6miSR4GzuvbPjZx7\n9qKrlTQnv2mqvlJVJ+6QbAHeUFW/3u3/KrCxqn5zqM+Brs9st/8VBjP6XcDtVXVj134tsLeqPjny\nM7YD27vdnwLuH8PYjnke8PAYX+9U4JhOfZM2HnBMp7ofr6qp+Tr1menPAucO7Z8DHDpOn9kkq4Ez\ngSM9z6WqdgO7e9SyYEmmq2rD0/Hay8UxnfombTzgmCZFnzX9/cC6JGuTnMbgg9k9I332ANu67YuB\n22rwT4g9wNbu7p61wDrg38ZTuiRpoead6Xdr9DuAfcAq4LqqOpBkFzBdVXuAa4EbkswwmOFv7c49\nkORm4F7gKPCOqnryaRqLJGkevR6tXFV7gb0jbZcPbT8ObDnOue8H3r+EGpfqaVk2WmaO6dQ3aeMB\nxzQR5v0gV5I0OXwMgyQ1ZGJDf75HR6xESQ4muSfJF5JML3c9i5HkuiQPJfniUNtzk3w6yZe7v5+z\nnDUu1HHGdEWSr3XX6gtJfnE5a1yoJOcm+ack9yU5kOS3uvYVea1OMJ4VfZ0WYyKXd7pHPXwJeB2D\n20b3A5dU1b3LWtgSJTkIbKiqFXtfcZJXAY8BH6uqn+7aPgAcqaoruzfo51TVu5azzoU4zpiuAB6r\nqg8uZ22LleQFwAuq6q4kPwzcCbwJuJQVeK1OMJ43s4Kv02JM6ky/z6MjtAyq6l8Y3OE1bPgxHtcz\n+GVcMY4zphWtqr5eVXd12/8D3Mfg2/Qr8lqdYDzNmdTQn+vREZNwgQv4xyR3dt9inhQ/WlVfh8Ev\nJ/D8Za5nXHYkubtb/lkRyyBz6Z6a+zPAHUzAtRoZD0zIdeprUkM/c7RNwjrWz1XVSxk88fQd3bKC\nTk1/CvwE8BLg68AfLW85i5PkWcAngd+uqm8tdz1LNcd4JuI6LcSkhn6vxz+sNFV1qPv7IeBTTM4T\nS7/RrbkeW3t9aJnrWbKq+kZVPVlVTwF/xgq8VkmewSAg/7Kq/qZrXrHXaq7xTMJ1WqhJDf0+j45Y\nUZKc0X0ARZIzgNcDXzzxWSvG8GM8tgF/t4y1jMWxYOz8MivsWnWPRr8WuK+qPjR0aEVeq+ONZ6Vf\np8WYyLt3ALpbrz7M/z06Yjm/FbxkSV7IYHYPg29Sf3wljinJXwGvZvB0w28A7wH+FrgZOA/4KrCl\nqlbMB6PHGdOrGSwZFHAQePuxtfCVIMkrgX8F7gGe6pp/j8E6+Iq7VicYzyWs4Ou0GBMb+pKk7zep\nyzuSpDkY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNeR/AR7qR/HzTDq0AAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "print(X_train.shape)\n",
    "print(Y_train.sum())\n",
    "print(X_test.shape)\n",
    "# tree = DecisionTreeClassifier(random_state=0, class_weight='balanced', min_weight_fraction_leaf=0.05,splitter='random')\n",
    "# tree.fit(X_train,Y_train)\n",
    "# tree.score(X_test,Y_test)\n",
    "# Y_predict = tree.predict(X_test)\n",
    "# doMetrics(Y_test,Y_predict,\"DecisionTreeClassifier\")\n",
    "# print(tree.feature_importances_)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=1000,n_jobs=4,class_weight='balanced', min_weight_fraction_leaf=0.1)\n",
    "rfc.fit(X_train,Y_train)\n",
    "doMetrics(Y_test,rfc.predict(X_test),\"RandomForestClassifier\")\n",
    "plt.bar(range(len(rfc.feature_importances_)),rfc.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 14}\n"
     ]
    }
   ],
   "source": [
    "#(n_estimators=1000,n_jobs=4,class_weight='balanced', min_weight_fraction_leaf=0.1\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\n",
    "          'max_depth':range(1,20,1)\n",
    "         }\n",
    "clf = RandomForestClassifier(class_weight='balanced',n_jobs=-1)\n",
    "gs = GridSearchCV(clf,params,cv=5,n_jobs=-1)\n",
    "gs.fit(X,Y)\n",
    "print(gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BalancedBaggingClassifier _Acc: 0.650319829424\n",
      "BalancedBaggingClassifier _recall: 0.6\n",
      "BalancedBaggingClassifier _f1-score: 0.267857142857\n",
      "[[275 144]\n",
      " [ 20  30]]\n"
     ]
    }
   ],
   "source": [
    "#使用降采样\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "bbc = BalancedBaggingClassifier(base_estimator=RandomForestClassifier(n_jobs=-1,max_depth=14,class_weight='balanced'),\n",
    "                                ratio='auto',\n",
    "                                replacement=False,\n",
    "                                random_state=0)\n",
    "bbc.fit(X_train, Y_train) \n",
    "\n",
    "y_pred = bbc.predict(X_test)\n",
    "confusion_matrix(Y_test, y_pred)\n",
    "doMetrics(Y_test,y_pred,\"BalancedBaggingClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-950d9576ea9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mplotTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tree\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "#对决策树进行画图和输出\n",
    "def plotTree(clf,clf_name,feature_names):\n",
    "    dot_data = export_graphviz(clf,\n",
    "                               out_file=None,\n",
    "                               feature_names=feature_names,\n",
    "                               class_names=[\"Good\",\"Bad\"],\n",
    "                               filled=True,\n",
    "                               rounded=True,\n",
    "                               special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    graph.write_pdf(clf_name + \".pdf\")\n",
    "plotTree(tree,\"tree\",X_train.columns.values)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
